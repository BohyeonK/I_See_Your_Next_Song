{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/work\n"
     ]
    }
   ],
   "source": [
    "cd /home/work/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2020 NVIDIA Corporation\n",
      "Built on Mon_Oct_12_20:09:46_PDT_2020\n",
      "Cuda compilation tools, release 11.1, V11.1.105\n",
      "Build cuda_11.1.TC455_06.29190527_0\n",
      "Wed Nov 23 11:42:57 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  CUDA GPU            Off  | 00000000:81:00.0 Off |                  Off |\n",
      "| 30%   26C    P8     5W / 230W |     48MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import os \n",
    "import gc\n",
    "import math\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import pickle\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import librosa\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.nn import functional as F\n",
    "import torchaudio.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "cudnn.benchmark = True\n",
    "plt.ion()   # 대화형 모드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('Tcae_apply/new_train.csv')\n",
    "valid = pd.read_csv('Tcae_apply/new_valid.csv')\n",
    "gender= pd.read_csv('Tcae_apply/all_gender.csv')\n",
    "\n",
    "train = train.merge(gender[['vocal', 'gender']], on='vocal', how='left')\n",
    "valid = valid.merge(gender[['vocal', 'gender']], on='vocal', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3825, 5), (675, 5), (4500, 3))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, valid.shape, gender.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vocal</th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>npy_path</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Final_dataset/정인_The End.wav</td>\n",
       "      <td>4412</td>\n",
       "      <td>정인_The End</td>\n",
       "      <td>/home/work/mel_dataset7/정인_The End.npy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Final_dataset/비_내여자 (Acoustic Ver.).wav</td>\n",
       "      <td>4111</td>\n",
       "      <td>비_내여자 (Acoustic Ver.)</td>\n",
       "      <td>/home/work/mel_dataset4/비_내여자 (Acoustic Ver.).npy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Final_dataset/폴킴_Goodbye Days.wav</td>\n",
       "      <td>4458</td>\n",
       "      <td>폴킴_Goodbye Days</td>\n",
       "      <td>/home/work/mel_dataset8/폴킴_Goodbye Days.npy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Final_dataset/스탠딩 에그_사랑한다는 말.wav</td>\n",
       "      <td>4242</td>\n",
       "      <td>스탠딩 에그_사랑한다는 말</td>\n",
       "      <td>/home/work/mel_dataset4/스탠딩 에그_사랑한다는 말.npy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Final_dataset/송가인_사랑의 꽃씨.wav</td>\n",
       "      <td>4479</td>\n",
       "      <td>송가인_사랑의 꽃씨</td>\n",
       "      <td>/home/work/mel_dataset4/송가인_사랑의 꽃씨.npy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     vocal  label                  title  \\\n",
       "0             Final_dataset/정인_The End.wav   4412             정인_The End   \n",
       "1  Final_dataset/비_내여자 (Acoustic Ver.).wav   4111  비_내여자 (Acoustic Ver.)   \n",
       "2        Final_dataset/폴킴_Goodbye Days.wav   4458        폴킴_Goodbye Days   \n",
       "3         Final_dataset/스탠딩 에그_사랑한다는 말.wav   4242         스탠딩 에그_사랑한다는 말   \n",
       "4             Final_dataset/송가인_사랑의 꽃씨.wav   4479             송가인_사랑의 꽃씨   \n",
       "\n",
       "                                            npy_path  gender  \n",
       "0             /home/work/mel_dataset7/정인_The End.npy       1  \n",
       "1  /home/work/mel_dataset4/비_내여자 (Acoustic Ver.).npy       0  \n",
       "2        /home/work/mel_dataset8/폴킴_Goodbye Days.npy       0  \n",
       "3         /home/work/mel_dataset4/스탠딩 에그_사랑한다는 말.npy       0  \n",
       "4             /home/work/mel_dataset4/송가인_사랑의 꽃씨.npy       1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import unicodedata\n",
    "\n",
    "train['lfcc_path'] = train['npy_path'].apply(lambda x : unicodedata.normalize('NFC', x.replace('mel_dataset', 'lfcc_dataset')))\n",
    "valid['lfcc_path'] = valid['npy_path'].apply(lambda x : unicodedata.normalize('NFC', x.replace('mel_dataset', 'lfcc_dataset')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vocal</th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>npy_path</th>\n",
       "      <th>gender</th>\n",
       "      <th>lfcc_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Final_dataset/정인_The End.wav</td>\n",
       "      <td>4412</td>\n",
       "      <td>정인_The End</td>\n",
       "      <td>/home/work/mel_dataset7/정인_The End.npy</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/work/lfcc_dataset7/정인_The End.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Final_dataset/비_내여자 (Acoustic Ver.).wav</td>\n",
       "      <td>4111</td>\n",
       "      <td>비_내여자 (Acoustic Ver.)</td>\n",
       "      <td>/home/work/mel_dataset4/비_내여자 (Acoustic Ver.).npy</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/work/lfcc_dataset4/비_내여자 (Acoustic Ver.)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Final_dataset/폴킴_Goodbye Days.wav</td>\n",
       "      <td>4458</td>\n",
       "      <td>폴킴_Goodbye Days</td>\n",
       "      <td>/home/work/mel_dataset8/폴킴_Goodbye Days.npy</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/work/lfcc_dataset8/폴킴_Goodbye Days.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Final_dataset/스탠딩 에그_사랑한다는 말.wav</td>\n",
       "      <td>4242</td>\n",
       "      <td>스탠딩 에그_사랑한다는 말</td>\n",
       "      <td>/home/work/mel_dataset4/스탠딩 에그_사랑한다는 말.npy</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/work/lfcc_dataset4/스탠딩 에그_사랑한다는 말.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Final_dataset/송가인_사랑의 꽃씨.wav</td>\n",
       "      <td>4479</td>\n",
       "      <td>송가인_사랑의 꽃씨</td>\n",
       "      <td>/home/work/mel_dataset4/송가인_사랑의 꽃씨.npy</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/work/lfcc_dataset4/송가인_사랑의 꽃씨.npy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     vocal  label                  title  \\\n",
       "0             Final_dataset/정인_The End.wav   4412             정인_The End   \n",
       "1  Final_dataset/비_내여자 (Acoustic Ver.).wav   4111  비_내여자 (Acoustic Ver.)   \n",
       "2        Final_dataset/폴킴_Goodbye Days.wav   4458        폴킴_Goodbye Days   \n",
       "3         Final_dataset/스탠딩 에그_사랑한다는 말.wav   4242         스탠딩 에그_사랑한다는 말   \n",
       "4             Final_dataset/송가인_사랑의 꽃씨.wav   4479             송가인_사랑의 꽃씨   \n",
       "\n",
       "                                            npy_path  gender  \\\n",
       "0             /home/work/mel_dataset7/정인_The End.npy       1   \n",
       "1  /home/work/mel_dataset4/비_내여자 (Acoustic Ver.).npy       0   \n",
       "2        /home/work/mel_dataset8/폴킴_Goodbye Days.npy       0   \n",
       "3         /home/work/mel_dataset4/스탠딩 에그_사랑한다는 말.npy       0   \n",
       "4             /home/work/mel_dataset4/송가인_사랑의 꽃씨.npy       1   \n",
       "\n",
       "                                           lfcc_path  \n",
       "0            /home/work/lfcc_dataset7/정인_The End.npy  \n",
       "1  /home/work/lfcc_dataset4/비_내여자 (Acoustic Ver.)...  \n",
       "2       /home/work/lfcc_dataset8/폴킴_Goodbye Days.npy  \n",
       "3        /home/work/lfcc_dataset4/스탠딩 에그_사랑한다는 말.npy  \n",
       "4            /home/work/lfcc_dataset4/송가인_사랑의 꽃씨.npy  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vocal</th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>npy_path</th>\n",
       "      <th>gender</th>\n",
       "      <th>lfcc_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Final_dataset/이효리_One Two Three N'Four.wav</td>\n",
       "      <td>4331</td>\n",
       "      <td>이효리_One Two Three N'Four</td>\n",
       "      <td>/home/work/mel_dataset6/이효리_One Two Three N'Fo...</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/work/lfcc_dataset6/이효리_One Two Three N'F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Final_dataset/소향_마지막 약속.wav</td>\n",
       "      <td>4490</td>\n",
       "      <td>소향_마지막 약속</td>\n",
       "      <td>/home/work/mel_dataset4/소향_마지막 약속.npy</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/work/lfcc_dataset4/소향_마지막 약속.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Final_dataset/박시환_고래의 꿈.wav</td>\n",
       "      <td>4449</td>\n",
       "      <td>박시환_고래의 꿈</td>\n",
       "      <td>/home/work/mel_dataset3/박시환_고래의 꿈.npy</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/work/lfcc_dataset3/박시환_고래의 꿈.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Final_dataset/신해철_재즈 카페.wav</td>\n",
       "      <td>4337</td>\n",
       "      <td>신해철_재즈 카페</td>\n",
       "      <td>/home/work/mel_dataset4/신해철_재즈 카페.npy</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/work/lfcc_dataset4/신해철_재즈 카페.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Final_dataset/자우림_Anna.wav</td>\n",
       "      <td>4418</td>\n",
       "      <td>자우림_Anna</td>\n",
       "      <td>/home/work/mel_dataset6/자우림_Anna.npy</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/work/lfcc_dataset6/자우림_Anna.npy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        vocal  label  \\\n",
       "0  Final_dataset/이효리_One Two Three N'Four.wav   4331   \n",
       "1                 Final_dataset/소향_마지막 약속.wav   4490   \n",
       "2                 Final_dataset/박시환_고래의 꿈.wav   4449   \n",
       "3                 Final_dataset/신해철_재즈 카페.wav   4337   \n",
       "4                  Final_dataset/자우림_Anna.wav   4418   \n",
       "\n",
       "                      title  \\\n",
       "0  이효리_One Two Three N'Four   \n",
       "1                 소향_마지막 약속   \n",
       "2                 박시환_고래의 꿈   \n",
       "3                 신해철_재즈 카페   \n",
       "4                  자우림_Anna   \n",
       "\n",
       "                                            npy_path  gender  \\\n",
       "0  /home/work/mel_dataset6/이효리_One Two Three N'Fo...       1   \n",
       "1              /home/work/mel_dataset4/소향_마지막 약속.npy       1   \n",
       "2              /home/work/mel_dataset3/박시환_고래의 꿈.npy       0   \n",
       "3              /home/work/mel_dataset4/신해철_재즈 카페.npy       0   \n",
       "4               /home/work/mel_dataset6/자우림_Anna.npy       1   \n",
       "\n",
       "                                           lfcc_path  \n",
       "0  /home/work/lfcc_dataset6/이효리_One Two Three N'F...  \n",
       "1             /home/work/lfcc_dataset4/소향_마지막 약속.npy  \n",
       "2             /home/work/lfcc_dataset3/박시환_고래의 꿈.npy  \n",
       "3             /home/work/lfcc_dataset4/신해철_재즈 카페.npy  \n",
       "4              /home/work/lfcc_dataset6/자우림_Anna.npy  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import unicodedata\n",
    "\n",
    "# lst = []\n",
    "\n",
    "# for path in tqdm(train['lfcc_path']):\n",
    "#     old_file = path\n",
    "#     new_file = unicodedata.normalize('NFC', path)\n",
    "#     try:\n",
    "#         os.rename(old_file, new_file)\n",
    "#     except:\n",
    "#         lst.append(old_file)\n",
    "        \n",
    "# for path in tqdm(valid['lfcc_path']):\n",
    "#     old_file = path\n",
    "#     new_file = unicodedata.normalize('NFC', path)\n",
    "#     try:\n",
    "#         os.rename(old_file, new_file)\n",
    "#     except:\n",
    "#         lst.append(old_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "\n",
    "# os.makedirs('data/train/male', exist_ok=True)\n",
    "# os.makedirs('data/train/female', exist_ok=True)\n",
    "# os.makedirs('data/valid/male', exist_ok=True)\n",
    "# os.makedirs('data/valid/female', exist_ok=True)\n",
    "\n",
    "# for i in tqdm(range(len(train['lfcc_path']))):\n",
    "#     file_source = train['lfcc_path'][i]\n",
    "#     if train['gender'][i]==0:\n",
    "#         file_destination = 'data/train/male'\n",
    "#     else:\n",
    "#         file_destination = 'data/train/female'\n",
    "#     shutil.copy(file_source, file_destination)\n",
    "\n",
    "# for i in tqdm(range(len(valid['lfcc_path']))):\n",
    "#     file_source = valid['lfcc_path'][i]\n",
    "#     if valid['gender'][i]==0:\n",
    "#         file_destination = 'data/valid/male'\n",
    "#     else:\n",
    "#         file_destination = 'data/valid/female'\n",
    "#     shutil.copy(file_source, file_destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/'\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Lambda(lambda image: image.convert('RGB')),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Lambda(lambda image: image.convert('RGB')),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "def npy_loader(path):\n",
    "    sample = torch.from_numpy(np.load(path))\n",
    "    return sample\n",
    "\n",
    "image_datasets = {x: datasets.DatasetFolder(root=os.path.join(data_dir, x),\n",
    "                                           loader=npy_loader,\n",
    "                                           extensions='.npy',\n",
    "                                           transform=data_transforms[x])\n",
    "                  for x in ['train', 'valid']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], \n",
    "                                              batch_size=16,\n",
    "                                              shuffle=True, \n",
    "                                              num_workers=0,\n",
    "                                              drop_last=True)\n",
    "              for x in ['train', 'valid']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'valid']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0353, 0.9216, 0.1451,  ..., 0.1961, 0.1961, 0.1961],\n",
       "          [0.3412, 0.1098, 0.6314,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.2941, 0.0784, 0.4510,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.5765, 0.0706, 0.0431,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.7059, 0.9255, 0.5529,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.6667, 0.8980, 0.3765,  ..., 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0353, 0.9216, 0.1451,  ..., 0.1961, 0.1961, 0.1961],\n",
       "          [0.3412, 0.1098, 0.6314,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.2941, 0.0784, 0.4510,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.5765, 0.0706, 0.0431,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.7059, 0.9255, 0.5529,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.6667, 0.8980, 0.3765,  ..., 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0353, 0.9216, 0.1451,  ..., 0.1961, 0.1961, 0.1961],\n",
       "          [0.3412, 0.1098, 0.6314,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.2941, 0.0784, 0.4510,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.5765, 0.0706, 0.0431,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.7059, 0.9255, 0.5529,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.6667, 0.8980, 0.3765,  ..., 0.0000, 0.0000, 0.0000]]]),\n",
       " 0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 256, 10336])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_datasets['train'][0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 학습하기\n",
    "- learning rate scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make conv_block\n",
    "def conv_block(shape, in_, out_, kernel_size, stride=(1, 1), padding=0):\n",
    "    block = torch.nn.Sequential(\n",
    "        nn.Conv2d(in_, out_, kernel_size, stride=stride, padding=padding, bias=False),\n",
    "        nn.ELU(),\n",
    "        nn.BatchNorm2d(out_),\n",
    "        nn.Dropout(0.2)\n",
    "    )\n",
    "    shape1 = int(np.floor((shape[0] - kernel_size[0] + 2 * padding) / stride[0]) + 1)\n",
    "    shape2 = int(np.floor((shape[1] - kernel_size[1] + 2 * padding) / stride[1]) + 1)\n",
    "    return block, shape1, shape2\n",
    "\n",
    "\n",
    "def fc_block(in_, out_):\n",
    "    block = torch.nn.Sequential(\n",
    "      torch.nn.Linear(in_, out_, bias=False),\n",
    "      torch.nn.BatchNorm1d(out_),\n",
    "      nn.ReLU()\n",
    "    )\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRNN2D_elu(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #self.Bn0 = nn.BatchNorm2d(input_size)\n",
    "        \n",
    "        self.conv_block1, self.shape1, self.shape2 = conv_block((256, 10336), 3, 8, (3, 3), padding=1)\n",
    "        self.conv_block2, self.shape1, self.shape2 = conv_block((self.shape1, self.shape2), 8, 16, (3, 3), stride=(2, 2))\n",
    "        self.conv_block3, self.shape1, self.shape2 = conv_block((self.shape1, self.shape2), 16, 32, (3, 3), stride=(2, 2))\n",
    "        self.conv_block4, self.shape1, self.shape2 = conv_block((self.shape1, self.shape2), 32, 64, (3, 3), stride=(2, 2))\n",
    "        self.conv_block5, self.shape1, self.shape2 = conv_block((self.shape1, self.shape2), 64, 128, (3, 3), stride=(1, 1))\n",
    "        self.conv_block6, self.shape1, self.shape2 = conv_block((self.shape1, self.shape2), 128, 256, (3, 3), stride=(2, 2))\n",
    "        self.conv_block7, self.shape1, self.shape2 = conv_block((self.shape1, self.shape2), 256, 512, (3, 3), stride=(2, 2))\n",
    "        self.conv_block8, self.shape1, self.shape2 = conv_block((self.shape1, self.shape2), 512, 512, (3, 3), stride=(4, 4))\n",
    "        \n",
    "        self.gru1 = nn.GRU(512, 64, num_layers=1, batch_first=True)\n",
    "        self.gru2 = nn.GRU(64, 64, num_layers=1, batch_first=True)\n",
    "        self.gru3 = nn.GRU(64, 64, num_layers=1, batch_first=True)\n",
    "        \n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        \n",
    "        self.fc_block1 = fc_block(5120, 64)\n",
    "        self.fc_last = nn.Linear(64, 2)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = torch.randn(1, x.size(0), 64).cuda()\n",
    "        \n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.conv_block3(x)\n",
    "        x = self.conv_block4(x)\n",
    "        x = self.conv_block5(x)\n",
    "        x = self.conv_block6(x)\n",
    "        x = self.conv_block7(x)\n",
    "        x = self.conv_block8(x)\n",
    "#         x = self.conv_block9(x)\n",
    "        \n",
    "        x = x.transpose(1, 3)\n",
    "        x = torch.reshape(x, (x.size(0), x.size(1), -1))\n",
    "        \n",
    "        x, h = self.gru1(x, h)\n",
    "        x, h = self.gru2(x, h)\n",
    "        x, h = self.gru3(x, h)\n",
    "        x = self.drop(x)\n",
    "        \n",
    "        x = torch.reshape(x, (x.size(0), -1))\n",
    "                \n",
    "        x = self.fc_block1(x)\n",
    "#         x = self.fc_block2(x)\n",
    "#         x = self.fc_block3(x)\n",
    "        x = self.fc_last(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "model_ft = CRNN2D_elu()\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 모든 매개변수들이 최적화되었는지 관찰\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# 7 에폭마다 0.1씩 학습률 감소\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1        [-1, 8, 256, 10336]             216\n",
      "               ELU-2        [-1, 8, 256, 10336]               0\n",
      "       BatchNorm2d-3        [-1, 8, 256, 10336]              16\n",
      "           Dropout-4        [-1, 8, 256, 10336]               0\n",
      "            Conv2d-5        [-1, 16, 127, 5167]           1,152\n",
      "               ELU-6        [-1, 16, 127, 5167]               0\n",
      "       BatchNorm2d-7        [-1, 16, 127, 5167]              32\n",
      "           Dropout-8        [-1, 16, 127, 5167]               0\n",
      "            Conv2d-9         [-1, 32, 63, 2583]           4,608\n",
      "              ELU-10         [-1, 32, 63, 2583]               0\n",
      "      BatchNorm2d-11         [-1, 32, 63, 2583]              64\n",
      "          Dropout-12         [-1, 32, 63, 2583]               0\n",
      "           Conv2d-13         [-1, 64, 31, 1291]          18,432\n",
      "              ELU-14         [-1, 64, 31, 1291]               0\n",
      "      BatchNorm2d-15         [-1, 64, 31, 1291]             128\n",
      "          Dropout-16         [-1, 64, 31, 1291]               0\n",
      "           Conv2d-17        [-1, 128, 29, 1289]          73,728\n",
      "              ELU-18        [-1, 128, 29, 1289]               0\n",
      "      BatchNorm2d-19        [-1, 128, 29, 1289]             256\n",
      "          Dropout-20        [-1, 128, 29, 1289]               0\n",
      "           Conv2d-21         [-1, 256, 14, 644]         294,912\n",
      "              ELU-22         [-1, 256, 14, 644]               0\n",
      "      BatchNorm2d-23         [-1, 256, 14, 644]             512\n",
      "          Dropout-24         [-1, 256, 14, 644]               0\n",
      "           Conv2d-25          [-1, 512, 6, 321]       1,179,648\n",
      "              ELU-26          [-1, 512, 6, 321]               0\n",
      "      BatchNorm2d-27          [-1, 512, 6, 321]           1,024\n",
      "          Dropout-28          [-1, 512, 6, 321]               0\n",
      "           Conv2d-29           [-1, 512, 1, 80]       2,359,296\n",
      "              ELU-30           [-1, 512, 1, 80]               0\n",
      "      BatchNorm2d-31           [-1, 512, 1, 80]           1,024\n",
      "          Dropout-32           [-1, 512, 1, 80]               0\n",
      "              GRU-33  [[-1, 80, 64], [-1, 2, 64]]               0\n",
      "              GRU-34  [[-1, 80, 64], [-1, 2, 64]]               0\n",
      "              GRU-35  [[-1, 80, 64], [-1, 2, 64]]               0\n",
      "          Dropout-36               [-1, 80, 64]               0\n",
      "           Linear-37                   [-1, 64]         327,680\n",
      "      BatchNorm1d-38                   [-1, 64]             128\n",
      "             ReLU-39                   [-1, 64]               0\n",
      "           Linear-40                    [-1, 2]             130\n",
      "================================================================\n",
      "Total params: 4,262,986\n",
      "Trainable params: 4,262,986\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 30.28\n",
      "Forward/backward pass size (MB): 1436.34\n",
      "Params size (MB): 16.26\n",
      "Estimated Total Size (MB): 1482.88\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model_ft, input_size=(3, 256, 10336))  # 40배 차이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "train_loader = dataloaders['train']\n",
    "valid_loader = dataloaders['valid']\n",
    "optimizer = optimizer_ft\n",
    "model = model_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d6c58b304db47af848a140b6ab92819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9de1d6cacefe4738adbbc4958b87ec8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/239 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1/20, train_acc : 0.5839, train_loss : 0.6940 / valid_acc : 0.5536, val_loss : 0.6846, takes 202.2200150489807secs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09ec06d94dc549bf90daca3e9f175cae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/239 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2/20, train_acc : 0.6187, train_loss : 0.6605 / valid_acc : 0.4881, val_loss : 0.6995, takes 182.3018093109131secs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76b1e24e457f47a190e6b29c9aa20f16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/239 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 3/20, train_acc : 0.6598, train_loss : 0.6307 / valid_acc : 0.5164, val_loss : 0.7119, takes 190.74477100372314secs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4fa4e187c6349a9b971590e0d707d07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/239 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 4/20, train_acc : 0.6789, train_loss : 0.5970 / valid_acc : 0.5223, val_loss : 0.7470, takes 194.81108164787292secs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89fb22d6f862468ca6155a2b1ba96581",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/239 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 5/20, train_acc : 0.7293, train_loss : 0.5367 / valid_acc : 0.5357, val_loss : 0.8081, takes 194.248685836792secs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1c80f85faf34f898834d57bdbbb3917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/239 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 6/20, train_acc : 0.7762, train_loss : 0.4752 / valid_acc : 0.5655, val_loss : 0.8119, takes 189.55034518241882secs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "968acc5099794dbe8fb82c5eb99ed819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/239 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 7/20, train_acc : 0.8203, train_loss : 0.4060 / valid_acc : 0.5342, val_loss : 0.8741, takes 189.3623652458191secs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9fa3f6fb94e4246b457ea9c96c6918d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/239 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 8/20, train_acc : 0.8342, train_loss : 0.3649 / valid_acc : 0.5491, val_loss : 0.8696, takes 185.96666479110718secs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "063a3f8d46b14732baf3a0db8b1795ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/239 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 9/20, train_acc : 0.8774, train_loss : 0.2979 / "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-842d5a2bc433>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mvalid_acc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mvalid_avg_loss\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0;31m# ------- assign valid data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \"\"\"\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mmode_to_nptype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'I'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'I;16'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'F'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     img = torch.from_numpy(\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode_to_nptype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     )\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss_arr = []\n",
    "valid_loss_arr = []\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    start = time.time()\n",
    "    train_avg_loss = 0\n",
    "    train_acc = 0\n",
    "    model.train()\n",
    "    for image, label in tqdm(train_loader):\n",
    "        # ------- assign train data\n",
    "        image = image.to(device)\n",
    "        label = label.to(device)\n",
    "        # ------- forward prop\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image)\n",
    "        # ------- backward prop\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # ------- get train performance\n",
    "        batch_acc = ((output.argmax(dim=1) == label).float().mean())\n",
    "        train_acc += batch_acc / len(train_loader)\n",
    "        train_avg_loss += loss / len(train_loader)\n",
    "    train_loss_arr.append(train_avg_loss)\n",
    "    print(f'Epoch : {epoch+1}/{epochs}, train_acc : {train_acc:.4f}, train_loss : {train_avg_loss:.4f}', end=' / ')\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_acc=0\n",
    "        valid_avg_loss =0\n",
    "        for image, label in valid_loader:\n",
    "            # ------- assign valid data\n",
    "            image = image.to(device)\n",
    "            label = label.to(device)\n",
    "            # ------- forward prop\n",
    "            val_output = model(image)\n",
    "            val_loss = criterion(val_output, label)\n",
    "            # ------- get valid performance\n",
    "            val_batch_acc = ((val_output.argmax(dim=1) == label).float().mean()) # acc = 맞춘 개수 / 배치사이즈\n",
    "            valid_acc += val_batch_acc / len(valid_loader) # acc / total_Iteration \n",
    "            valid_avg_loss += val_loss / len(valid_loader) # val_loss / total_Iteration\n",
    "        valid_loss_arr.append(valid_avg_loss) \n",
    "        print(f'valid_acc : {valid_acc:.4f}, val_loss : {valid_avg_loss:.4f}, takes {time.time() - start}secs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss_arr, label='train')\n",
    "plt.plot(valid_loss_arr, label='valid')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_ft.state_dict(), 'resnet50.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load('Tcae_apply/gender_classification_model.pt')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'male'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from torch.autograd import Variable\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "dic = {'0': 'male', '1' : 'female'}\n",
    "img_array = np.load('mel_dataset5/안예은_가자.npy').squeeze(0)\n",
    "\n",
    "transformation = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Lambda(lambda image: image.convert('RGB')),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "image_tensor = transformation(img_array).float()\n",
    "image_tensor = image_tensor.unsqueeze_(0)\n",
    "\n",
    "img = Variable(image_tensor).to(device)\n",
    "output = model(img).to(device)\n",
    "index = output.cpu().data.numpy().argmax()\n",
    "\n",
    "dic[str(index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 1.9.0 on Python 3.8 (CUDA 11.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
