{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/work\n"
     ]
    }
   ],
   "source": [
    "cd /home/work/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import torch \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re \n",
    "import os \n",
    "from torch.nn import functional as F\n",
    "import torchaudio.transforms as T\n",
    "import torchaudio.functional as F\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "import gc\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import warnings\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7efee00712f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Audio\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import unicodedata\n",
    "\n",
    "torch.random.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0+cu111\n",
      "0.9.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.functional as F\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/home/work/'\n",
    "dataset1 = [base_dir+'dataset1/'+x for x in os.listdir(base_dir+'dataset1')]\n",
    "dataset2 = [base_dir+'dataset2/'+x for x in os.listdir(base_dir+'dataset2')]\n",
    "dataset3 = [base_dir+'dataset3/'+x for x in os.listdir(base_dir+'dataset3')]\n",
    "dataset4 = [base_dir+'dataset4/'+x for x in os.listdir(base_dir+'dataset4')]\n",
    "dataset5 = [base_dir+'dataset5/'+x for x in os.listdir(base_dir+'dataset5')]\n",
    "dataset6 = [base_dir+'dataset6/'+x for x in os.listdir(base_dir+'dataset6')]\n",
    "dataset7 = [base_dir+'dataset7/'+x for x in os.listdir(base_dir+'dataset7')]\n",
    "dataset8 = [base_dir+'dataset8/'+x for x in os.listdir(base_dir+'dataset8')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import warnings\n",
    "from typing import Callable, Optional, Union\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn.modules.lazy import LazyModuleMixin\n",
    "from torch.nn.parameter import UninitializedParameter\n",
    "\n",
    "from torchaudio import functional as F\n",
    "\n",
    "\n",
    "class Spectrogram(torch.nn.Module):\n",
    "    r\"\"\"Create a spectrogram from a audio signal.\n",
    "    .. devices:: CPU CUDA\n",
    "    .. properties:: Autograd TorchScript\n",
    "    Args:\n",
    "        n_fft (int, optional): Size of FFT, creates ``n_fft // 2 + 1`` bins. (Default: ``400``)\n",
    "        win_length (int or None, optional): Window size. (Default: ``n_fft``)\n",
    "        hop_length (int or None, optional): Length of hop between STFT windows. (Default: ``win_length // 2``)\n",
    "        pad (int, optional): Two sided padding of signal. (Default: ``0``)\n",
    "        window_fn (Callable[..., Tensor], optional): A function to create a window tensor\n",
    "            that is applied/multiplied to each frame/window. (Default: ``torch.hann_window``)\n",
    "        power (float or None, optional): Exponent for the magnitude spectrogram,\n",
    "            (must be > 0) e.g., 1 for energy, 2 for power, etc.\n",
    "            If None, then the complex spectrum is returned instead. (Default: ``2``)\n",
    "        normalized (bool or str, optional): Whether to normalize by magnitude after stft. If input is str, choices are\n",
    "            ``\"window\"`` and ``\"frame_length\"``, if specific normalization type is desirable. ``True`` maps to\n",
    "            ``\"window\"``. (Default: ``False``)\n",
    "        wkwargs (dict or None, optional): Arguments for window function. (Default: ``None``)\n",
    "        center (bool, optional): whether to pad :attr:`waveform` on both sides so\n",
    "            that the :math:`t`-th frame is centered at time :math:`t \\times \\text{hop\\_length}`.\n",
    "            (Default: ``True``)\n",
    "        pad_mode (string, optional): controls the padding method used when\n",
    "            :attr:`center` is ``True``. (Default: ``\"reflect\"``)\n",
    "        onesided (bool, optional): controls whether to return half of results to\n",
    "            avoid redundancy (Default: ``True``)\n",
    "        return_complex (bool, optional):\n",
    "            Deprecated and not used.\n",
    "    Example\n",
    "        >>> waveform, sample_rate = torchaudio.load(\"test.wav\", normalize=True)\n",
    "        >>> transform = torchaudio.transforms.Spectrogram(n_fft=800)\n",
    "        >>> spectrogram = transform(waveform)\n",
    "    \"\"\"\n",
    "    __constants__ = [\"n_fft\", \"win_length\", \"hop_length\", \"pad\", \"power\", \"normalized\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_fft: int = 400,\n",
    "        win_length: Optional[int] = None,\n",
    "        hop_length: Optional[int] = None,\n",
    "        pad: int = 0,\n",
    "        window_fn: Callable[..., Tensor] = torch.hann_window,\n",
    "        power: Optional[float] = 2.0,\n",
    "        normalized: Union[bool, str] = False,\n",
    "        wkwargs: Optional[dict] = None,\n",
    "        center: bool = True,\n",
    "        pad_mode: str = \"reflect\",\n",
    "        onesided: bool = True,\n",
    "        return_complex: Optional[bool] = None,\n",
    "    ) -> None:\n",
    "        super(Spectrogram, self).__init__()\n",
    "        torch._C._log_api_usage_once(\"torchaudio.transforms.Spectrogram\")\n",
    "        self.n_fft = n_fft\n",
    "        # number of FFT bins. the returned STFT result will have n_fft // 2 + 1\n",
    "        # number of frequencies due to onesided=True in torch.stft\n",
    "        self.win_length = win_length if win_length is not None else n_fft\n",
    "        self.hop_length = hop_length if hop_length is not None else self.win_length // 2\n",
    "        window = window_fn(self.win_length) if wkwargs is None else window_fn(self.win_length, **wkwargs)\n",
    "        self.register_buffer(\"window\", window)\n",
    "        self.pad = pad\n",
    "        self.power = power\n",
    "        self.normalized = normalized\n",
    "        self.center = center\n",
    "        self.pad_mode = pad_mode\n",
    "        self.onesided = onesided\n",
    "        if return_complex is not None:\n",
    "            warnings.warn(\n",
    "                \"`return_complex` argument is now deprecated and is not effective.\"\n",
    "                \"`torchaudio.transforms.Spectrogram(power=None)` always returns a tensor with \"\n",
    "                \"complex dtype. Please remove the argument in the function call.\"\n",
    "            )\n",
    "\n",
    "    def forward(self, waveform: Tensor) -> Tensor:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            waveform (Tensor): Tensor of audio of dimension (..., time).\n",
    "        Returns:\n",
    "            Tensor: Dimension (..., freq, time), where freq is\n",
    "            ``n_fft // 2 + 1`` where ``n_fft`` is the number of\n",
    "            Fourier bins, and time is the number of window hops (n_frame).\n",
    "        \"\"\"\n",
    "        return F.spectrogram(\n",
    "            waveform,\n",
    "            self.pad,\n",
    "            self.window,\n",
    "            self.n_fft,\n",
    "            self.hop_length,\n",
    "            self.win_length,\n",
    "            self.power,\n",
    "            self.normalized,\n",
    "            self.center,\n",
    "            self.pad_mode,\n",
    "            self.onesided,\n",
    "        )\n",
    "\n",
    "\n",
    "class AmplitudeToDB(torch.nn.Module):\n",
    "    r\"\"\"Turn a tensor from the power/amplitude scale to the decibel scale.\n",
    "    .. devices:: CPU CUDA\n",
    "    .. properties:: Autograd TorchScript\n",
    "    This output depends on the maximum value in the input tensor, and so\n",
    "    may return different values for an audio clip split into snippets vs. a\n",
    "    a full clip.\n",
    "    Args:\n",
    "        stype (str, optional): scale of input tensor (``\"power\"`` or ``\"magnitude\"``). The\n",
    "            power being the elementwise square of the magnitude. (Default: ``\"power\"``)\n",
    "        top_db (float or None, optional): minimum negative cut-off in decibels.  A reasonable\n",
    "            number is 80. (Default: ``None``)\n",
    "    Example\n",
    "        >>> waveform, sample_rate = torchaudio.load(\"test.wav\", normalize=True)\n",
    "        >>> transform = transforms.AmplitudeToDB(stype=\"amplitude\", top_db=80)\n",
    "        >>> waveform_db = transform(waveform)\n",
    "    \"\"\"\n",
    "    __constants__ = [\"multiplier\", \"amin\", \"ref_value\", \"db_multiplier\"]\n",
    "\n",
    "    def __init__(self, stype: str = \"power\", top_db: Optional[float] = None) -> None:\n",
    "        super(AmplitudeToDB, self).__init__()\n",
    "        self.stype = stype\n",
    "        if top_db is not None and top_db < 0:\n",
    "            raise ValueError(\"top_db must be positive value\")\n",
    "        self.top_db = top_db\n",
    "        self.multiplier = 10.0 if stype == \"power\" else 20.0\n",
    "        self.amin = 1e-10\n",
    "        self.ref_value = 1.0\n",
    "        self.db_multiplier = math.log10(max(self.amin, self.ref_value))\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        r\"\"\"Numerically stable implementation from Librosa.\n",
    "        https://librosa.org/doc/latest/generated/librosa.amplitude_to_db.html\n",
    "        Args:\n",
    "            x (Tensor): Input tensor before being converted to decibel scale.\n",
    "        Returns:\n",
    "            Tensor: Output tensor in decibel scale.\n",
    "        \"\"\"\n",
    "        return F.amplitude_to_DB(x, self.multiplier, self.amin, self.db_multiplier, self.top_db)\n",
    "\n",
    "\n",
    "def linear_fbanks(\n",
    "    n_freqs: int,\n",
    "    f_min: float,\n",
    "    f_max: float,\n",
    "    n_filter: int,\n",
    "    sample_rate: int,\n",
    ") -> Tensor:\n",
    "    r\"\"\"Creates a linear triangular filterbank.\n",
    "    .. devices:: CPU\n",
    "    .. properties:: TorchScript\n",
    "    Note:\n",
    "        For the sake of the numerical compatibility with librosa, not all the coefficients\n",
    "        in the resulting filter bank has magnitude of 1.\n",
    "        .. image:: https://download.pytorch.org/torchaudio/doc-assets/lin_fbanks.png\n",
    "           :alt: Visualization of generated filter bank\n",
    "    Args:\n",
    "        n_freqs (int): Number of frequencies to highlight/apply\n",
    "        f_min (float): Minimum frequency (Hz)\n",
    "        f_max (float): Maximum frequency (Hz)\n",
    "        n_filter (int): Number of (linear) triangular filter\n",
    "        sample_rate (int): Sample rate of the audio waveform\n",
    "    Returns:\n",
    "        Tensor: Triangular filter banks (fb matrix) of size (``n_freqs``, ``n_filter``)\n",
    "        meaning number of frequencies to highlight/apply to x the number of filterbanks.\n",
    "        Each column is a filterbank so that assuming there is a matrix A of\n",
    "        size (..., ``n_freqs``), the applied result would be\n",
    "        ``A * linear_fbanks(A.size(-1), ...)``.\n",
    "    \"\"\"\n",
    "    # freq bins\n",
    "    all_freqs = torch.linspace(0, sample_rate // 2, n_freqs)\n",
    "\n",
    "    # filter mid-points\n",
    "    f_pts = torch.linspace(f_min, f_max, n_filter + 2)\n",
    "\n",
    "    # create filterbank\n",
    "    fb = _create_triangular_filterbank(all_freqs, f_pts)\n",
    "\n",
    "    return fb\n",
    "\n",
    "\n",
    "def _create_triangular_filterbank(\n",
    "    all_freqs: Tensor,\n",
    "    f_pts: Tensor,\n",
    ") -> Tensor:\n",
    "    \"\"\"Create a triangular filter bank.\n",
    "    Args:\n",
    "        all_freqs (Tensor): STFT freq points of size (`n_freqs`).\n",
    "        f_pts (Tensor): Filter mid points of size (`n_filter`).\n",
    "    Returns:\n",
    "        fb (Tensor): The filter bank of size (`n_freqs`, `n_filter`).\n",
    "    \"\"\"\n",
    "    # Adopted from Librosa\n",
    "    # calculate the difference between each filter mid point and each stft freq point in hertz\n",
    "    f_diff = f_pts[1:] - f_pts[:-1]  # (n_filter + 1)\n",
    "    slopes = f_pts.unsqueeze(0) - all_freqs.unsqueeze(1)  # (n_freqs, n_filter + 2)\n",
    "    # create overlapping triangles\n",
    "    zero = torch.zeros(1)\n",
    "    down_slopes = (-1.0 * slopes[:, :-2]) / f_diff[:-1]  # (n_freqs, n_filter)\n",
    "    up_slopes = slopes[:, 2:] / f_diff[1:]  # (n_freqs, n_filter)\n",
    "    fb = torch.max(zero, torch.min(down_slopes, up_slopes))\n",
    "\n",
    "    return fb\n",
    "\n",
    "\n",
    "class LFCC(torch.nn.Module):\n",
    "    r\"\"\"Create the linear-frequency cepstrum coefficients from an audio signal.\n",
    "    .. devices:: CPU CUDA\n",
    "    .. properties:: Autograd TorchScript\n",
    "    By default, this calculates the LFCC on the DB-scaled linear filtered spectrogram.\n",
    "    This is not the textbook implementation, but is implemented here to\n",
    "    give consistency with librosa.\n",
    "    This output depends on the maximum value in the input spectrogram, and so\n",
    "    may return different values for an audio clip split into snippets vs. a\n",
    "    a full clip.\n",
    "    Args:\n",
    "        sample_rate (int, optional): Sample rate of audio signal. (Default: ``16000``)\n",
    "        n_filter (int, optional): Number of linear filters to apply. (Default: ``128``)\n",
    "        n_lfcc (int, optional): Number of lfc coefficients to retain. (Default: ``40``)\n",
    "        f_min (float, optional): Minimum frequency. (Default: ``0.``)\n",
    "        f_max (float or None, optional): Maximum frequency. (Default: ``None``)\n",
    "        dct_type (int, optional): type of DCT (discrete cosine transform) to use. (Default: ``2``)\n",
    "        norm (str, optional): norm to use. (Default: ``\"ortho\"``)\n",
    "        log_lf (bool, optional): whether to use log-lf spectrograms instead of db-scaled. (Default: ``False``)\n",
    "        speckwargs (dict or None, optional): arguments for Spectrogram. (Default: ``None``)\n",
    "    Example\n",
    "        >>> waveform, sample_rate = torchaudio.load(\"test.wav\", normalize=True)\n",
    "        >>> transform = transforms.LFCC(\n",
    "        >>>     sample_rate=sample_rate,\n",
    "        >>>     n_lfcc=13,\n",
    "        >>>     speckwargs={\"n_fft\": 400, \"hop_length\": 160, \"center\": False},\n",
    "        >>> )\n",
    "        >>> lfcc = transform(waveform)\n",
    "    See also:\n",
    "        :py:func:`torchaudio.functional.linear_fbanks` - The function used to\n",
    "        generate the filter banks.\n",
    "    \"\"\"\n",
    "    __constants__ = [\"sample_rate\", \"n_filter\", \"n_lfcc\", \"dct_type\", \"top_db\", \"log_lf\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sample_rate: int = 16000,\n",
    "        n_filter: int = 128,\n",
    "        f_min: float = 0.0,\n",
    "        f_max: Optional[float] = None,\n",
    "        n_lfcc: int = 40,\n",
    "        dct_type: int = 2,\n",
    "        norm: str = \"ortho\",\n",
    "        log_lf: bool = False,\n",
    "        speckwargs: Optional[dict] = None,\n",
    "    ) -> None:\n",
    "        super(LFCC, self).__init__()\n",
    "        supported_dct_types = [2]\n",
    "        if dct_type not in supported_dct_types:\n",
    "            raise ValueError(\"DCT type not supported: {}\".format(dct_type))\n",
    "        self.sample_rate = sample_rate\n",
    "        self.f_min = f_min\n",
    "        self.f_max = f_max if f_max is not None else float(sample_rate // 2)\n",
    "        self.n_filter = n_filter\n",
    "        self.n_lfcc = n_lfcc\n",
    "        self.dct_type = dct_type\n",
    "        self.norm = norm\n",
    "        self.top_db = 80.0\n",
    "        self.amplitude_to_DB = AmplitudeToDB(\"power\", self.top_db)\n",
    "\n",
    "        speckwargs = speckwargs or {}\n",
    "        self.Spectrogram = Spectrogram(**speckwargs)\n",
    "\n",
    "        if self.n_lfcc > self.Spectrogram.n_fft:\n",
    "            raise ValueError(\"Cannot select more LFCC coefficients than # fft bins\")\n",
    "\n",
    "        filter_mat = linear_fbanks(\n",
    "            n_freqs=self.Spectrogram.n_fft // 2 + 1,\n",
    "            f_min=self.f_min,\n",
    "            f_max=self.f_max,\n",
    "            n_filter=self.n_filter,\n",
    "            sample_rate=self.sample_rate,\n",
    "        )\n",
    "        self.register_buffer(\"filter_mat\", filter_mat)\n",
    "\n",
    "        dct_mat = F.create_dct(self.n_lfcc, self.n_filter, self.norm)\n",
    "        self.register_buffer(\"dct_mat\", dct_mat)\n",
    "        self.log_lf = log_lf\n",
    "\n",
    "    def forward(self, waveform: Tensor) -> Tensor:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            waveform (Tensor): Tensor of audio of dimension (..., time).\n",
    "        Returns:\n",
    "            Tensor: Linear Frequency Cepstral Coefficients of size (..., ``n_lfcc``, time).\n",
    "        \"\"\"\n",
    "        specgram = self.Spectrogram(waveform)\n",
    "\n",
    "        # (..., time, freq) dot (freq, n_filter) -> (..., n_filter, time)\n",
    "        specgram = torch.matmul(specgram.transpose(-1, -2), self.filter_mat).transpose(-1, -2)\n",
    "\n",
    "        if self.log_lf:\n",
    "            log_offset = 1e-6\n",
    "            specgram = torch.log(specgram + log_offset)\n",
    "        else:\n",
    "            specgram = self.amplitude_to_DB(specgram)\n",
    "\n",
    "        # (..., time, n_filter) dot (n_filter, n_lfcc) -> (..., n_lfcc, time)\n",
    "        lfcc = torch.matmul(specgram.transpose(-1, -2), self.dct_mat).transpose(-1, -2)\n",
    "        return lfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fft = 2048\n",
    "win_length = None\n",
    "hop_length = 512\n",
    "n_lfcc = 256\n",
    "\n",
    "def make_lfcc(song_paths, save_dir, sample_rate=44100): # song_paths = dataset1\n",
    "    \n",
    "    for j in tqdm(range(len(song_paths))):\n",
    "        wav, wav_sample_rate = torchaudio.load(song_paths[j])\n",
    "          \n",
    "        if wav_sample_rate != sample_rate:\n",
    "            transform = torchaudio.transforms.Resample(wav_sample_rate, sample_rate)\n",
    "            wav = transform(wav)\n",
    "                \n",
    "        wav = torch.unsqueeze(torch.mean(wav, axis=0), dim=0) # mono\n",
    "        wav_to_lfcc = LFCC(\n",
    "            sample_rate=sample_rate,\n",
    "            n_lfcc=n_lfcc,\n",
    "            speckwargs={\n",
    "                \"n_fft\": n_fft,\n",
    "                \"win_length\": win_length,\n",
    "                \"hop_length\": hop_length,\n",
    "                },\n",
    "        )\n",
    "\n",
    "        lfcc = wav_to_lfcc(wav)\n",
    "        lfcc = lfcc.cpu().detach().numpy() # to numpy\n",
    "        song_name = os.path.splitext(song_paths[j])[0].split('/')[-1]\n",
    "        song_name = unicodedata.normalize('NFC', song_name)\n",
    "        np.save(f\"{save_dir}/{song_name}.npy\", lfcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs('lfcc_dataset1', exist_ok=True)\n",
    "# os.makedirs('lfcc_dataset2', exist_ok=True)\n",
    "# os.makedirs('lfcc_dataset3', exist_ok=True)\n",
    "# os.makedirs('lfcc_dataset4', exist_ok=True)\n",
    "# os.makedirs('lfcc_dataset5', exist_ok=True)\n",
    "# os.makedirs('lfcc_dataset6', exist_ok=True)\n",
    "# os.makedirs('lfcc_dataset7', exist_ok=True)\n",
    "# os.makedirs('lfcc_dataset8', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1c2e344b3524d34b9384284c2104f36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21b1429e9921474aa36a5e35b90de9f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/540 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasets = [dataset1, dataset2, dataset3, dataset4, dataset5, dataset6, dataset7, dataset8]\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    save_dir = f\"{base_dir}lfcc_dataset{i+1}\"\n",
    "    make_lfcc(song_paths=dataset, save_dir=save_dir, sample_rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 256, 10336)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load('lfcc_dataset1/10CM_그대와 나.npy').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 1.9.0 on Python 3.8 (CUDA 11.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
