{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import torch \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re \n",
    "import os \n",
    "from torch.nn import functional as F\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "import gc\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/home/work/'\n",
    "np1 = glob.glob(base_dir+'mel_dataset1/*.npy')\n",
    "np2 = glob.glob(base_dir+'mel_dataset2/*.npy')\n",
    "np3 = glob.glob(base_dir+'mel_dataset3/*.npy')\n",
    "np4 = glob.glob(base_dir+'mel_dataset4/*.npy')\n",
    "np5 = glob.glob(base_dir+'mel_dataset5/*.npy')\n",
    "np6 = glob.glob(base_dir+'mel_dataset6/*.npy')\n",
    "np7 = glob.glob(base_dir+'mel_dataset7/*.npy')\n",
    "np8 = glob.glob(base_dir+'mel_dataset8/*.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = np1+np2+np3+np4+np5+np6+np7+np8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv('new_train.csv')\n",
    "valid_csv = pd.read_csv('new_valid.csv')\n",
    "all_csv =pd.read_csv('new_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6341af5fa0a14195bfc9a7bd4e0ac362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3825 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5511ab959e7c48f5bc0a13e4e7bd57bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/675 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def npy_list(csv):\n",
    "    npy_list = []\n",
    "    for song_path in tqdm(csv['npy_path']):\n",
    "        song_npy = np.load(song_path).squeeze()\n",
    "        #npy_list.append(song_npy[-1876:])\n",
    "        npy_list.append(song_npy)\n",
    "        \n",
    "    return npy_list\n",
    "\n",
    "train_list = [np.load(song_path).squeeze() for song_path in tqdm(train_csv['npy_path'])]\n",
    "valid_list = [np.load(song_path).squeeze() for song_path in tqdm(valid_csv['npy_path'])]\n",
    "#train_list = npy_list(train_csv)\n",
    "#valid_list = npy_list(valid_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_list = train_list + valid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "print(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# epoch문 안에 들어가야되나?\n",
    "#train_batch_li = DataLoader(train_list, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "#val_batch_li = DataLoader(valid_list, batch_size=batch_size, shuffle=True,drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in train_loader: \n",
    "        \n",
    "        \n",
    "        mel = torch.FloatTensor(batch).to(DEVICE)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        encode, output = model(mel)\n",
    "        \n",
    "        loss = criterion(output, mel)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    train_loss /= (len(train_loader))\n",
    "\n",
    "    return train_loss\n",
    "\n",
    "def val(model, train_loader):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in train_loader:\n",
    "            \n",
    "            \n",
    "            mel = torch.FloatTensor(batch).to(DEVICE)\n",
    "            \n",
    "            encode, output = model(mel)\n",
    "\n",
    "            loss = criterion(output, mel)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= (len(train_loader))\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "def get_mel_embeding(model, train_loader):\n",
    "    model.eval()\n",
    "    mel_embeding_li = []\n",
    "    with torch.no_grad():\n",
    "        for batch in train_loader: \n",
    "            \n",
    "            mel = torch.FloatTensor(batch).to(DEVICE)\n",
    "            \n",
    "            encode, output = model(mel)\n",
    "            mel_embeding_li.append(encode.detach().cpu().numpy())\n",
    "\n",
    "    return mel_embeding_li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#residual 을 어디다가 이어야될까..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# 이거아님\\n############ 원래 ##############\\nclass TimeAutoEncoder(nn.Module):\\n    def __init__(self):\\n        super(TimeAutoEncoder, self).__init__()\\n        self.conv1 = nn.Sequential(\\n            nn.Conv1d(in_channels = 48, out_channels = 512, kernel_size = 3, stride = 1, padding = 0, dilation = 1),\\n            #nn.BatchNorm1d(512),\\n            nn.ReLU(),\\n        )\\n    \\n        self.conv2 = nn.Sequential(\\n            nn.Conv1d(in_channels = 512, out_channels = 256, kernel_size = 3, stride = 1, padding = 0, dilation = 2),\\n            #nn.BatchNorm1d(256),\\n            nn.ReLU(),\\n        )\\n            \\n        self.conv3 = nn.Sequential(\\n            nn.Conv1d(in_channels = 256, out_channels = 64, kernel_size = 3, stride = 1, padding = 0, dilation = 4),\\n            #nn.BatchNorm1d(128),\\n            nn.ReLU(),\\n        )\\n\\n        self.conv4 = nn.Sequential(\\n            nn.Conv1d(in_channels = 128, out_channels = 32, kernel_size = 3, stride = 1, padding = 0, dilation = 8),\\n            #nn.BatchNorm1d(64),\\n            nn.ReLU(),\\n        )\\n\\n        self.conv5 = nn.Sequential(\\n            nn.Conv1d(in_channels = 64, out_channels = 16, kernel_size = 3, stride = 1, padding = 0, dilation = 16),\\n            #nn.BatchNorm1d(32),\\n            nn.ReLU(),\\n        )\\n\\n        self.conv6 = nn.Sequential(\\n            nn.Conv1d(in_channels = 32, out_channels = 8, kernel_size = 3, stride = 1, padding = 0, dilation = 32),\\n            #nn.BatchNorm1d(16),\\n            nn.ReLU(),\\n        )\\n\\n        self.conv7 = nn.Sequential(\\n            nn.Conv1d(in_channels = 16, out_channels = 4, kernel_size = 3, stride = 1, padding = 0, dilation = 64),\\n            #nn.BatchNorm1d(8),\\n            nn.ReLU(),\\n        )\\n\\n        self.encoder_fc = nn.Sequential(\\n            nn.Linear(8 * 7501, 256),\\n            #nn.BatchNorm1d(256),\\n            nn.Tanh(),\\n            \\n        )\\n        \\n        self.decoder_fc = nn.Sequential(\\n            nn.Linear(256, 8 * 7501),\\n            nn.ReLU(),\\n        )\\n\\n        self.t_conv1 = nn.Sequential(\\n            # nn.ConvTranspose1d(in_channels = 8, out_channels = 16, kernel_size  = 3, stride = 1, dilation=62),\\n            nn.Conv1d(in_channels = 16, out_channels = 16, kernel_size = 3, stride = 1, padding = 0, dilation = 64),\\n            #nn.BatchNorm1d(16),\\n            nn.ReLU(),\\n        )\\n\\n        self.t_conv2 = nn.Sequential(\\n            # nn.ConvTranspose1d(in_channels = 16, out_channels = 32, kernel_size  = 3, stride = 1, dilation = 30),\\n            nn.Conv1d(in_channels = 32, out_channels = 32, kernel_size = 3, stride = 1, padding = 0, dilation = 32),\\n            #nn.BatchNorm1d(32),\\n            nn.ReLU(),\\n        )\\n\\n        self.t_conv3 = nn.Sequential(\\n            # nn.ConvTranspose1d(in_channels = 32, out_channels = 64, kernel_size  = 3, stride = 1, dilation=14),\\n            nn.Conv1d(in_channels = 64, out_channels = 64, kernel_size = 3, stride = 1, padding = 0, dilation = 16),\\n            #nn.BatchNorm1d(64),\\n            nn.ReLU(),\\n        )\\n\\n        self.t_conv4 = nn.Sequential(\\n            # nn.ConvTranspose1d(in_channels = 64, out_channels = 128, kernel_size  = 3, stride = 1, dilation = 6),\\n            nn.Conv1d(in_channels = 128, out_channels = 128, kernel_size = 3, stride = 1, padding = 0, dilation = 8),\\n            #nn.BatchNorm1d(128),\\n            nn.ReLU(),\\n        )\\n\\n        self.t_conv5 = nn.Sequential(\\n            # nn.ConvTranspose1d(in_channels = 128, out_channels = 256, kernel_size  = 3, stride = 1, dilation=2),\\n            nn.Conv1d(in_channels = 256, out_channels = 256, kernel_size = 3, stride = 1, padding = 0, dilation = 4),\\n            #nn.BatchNorm1d(256),\\n            nn.ReLU(),\\n        )\\n\\n        self.t_conv6 = nn.Sequential(\\n            # nn.ConvTranspose1d(in_channels = 256, out_channels = 512, kernel_size  = 3, stride = 1, dilation = 1),\\n            nn.Conv1d(in_channels = 512, out_channels = 512, kernel_size = 3, stride = 1, padding = 0, dilation = 2),\\n            #nn.BatchNorm1d(512),\\n            nn.ReLU(),\\n        )\\n\\n        self.t_conv7 = nn.Sequential(\\n            # nn.ConvTranspose1d(in_channels = 512, out_channels = 48, kernel_size  = 3, stride = 1, dilation= 1),\\n            nn.Conv1d(in_channels = 1024, out_channels = 48, kernel_size = 3, stride = 1, padding = 0, dilation = 1)\\n        )\\n\\n        \\n        \\n        \\n        self.conv1to3 = nn.Sequential(\\n            nn.Conv1d(in_channels = 512, out_channels = 64, kernel_size = 3, stride = 1, padding = 0, dilation = 4),\\n            #nn.BatchNorm1d(512),\\n            nn.ReLU(),\\n        )# out : 128 * 7501\\n        \\n        self.conv2to4 = nn.Sequential(\\n            nn.Conv1d(in_channels = 256, out_channels = 32, kernel_size = 3, stride = 1, padding = 0, dilation = 8),\\n            #nn.BatchNorm1d(512),\\n            nn.ReLU(),\\n        )# out : 64 * 7501\\n        \\n        self.conv3to5 = nn.Sequential(\\n            nn.Conv1d(in_channels = 128, out_channels = 16, kernel_size = 3, stride = 1, padding = 0, dilation = 16),\\n            #nn.BatchNorm1d(512),\\n            nn.ReLU(),\\n        )# out : 32 * 7501\\n        \\n        self.conv4to6 = nn.Sequential(\\n            nn.Conv1d(in_channels = 64, out_channels = 8, kernel_size = 3, stride = 1, padding = 0, dilation = 32),\\n            #nn.BatchNorm1d(512),\\n            nn.ReLU(),\\n        )# out : 16 * 7501\\n        \\n        self.conv5to7 = nn.Sequential(\\n            nn.Conv1d(in_channels = 32, out_channels = 4, kernel_size = 3, stride = 1, padding = 0, dilation = 64),\\n            #nn.BatchNorm1d(512),\\n            nn.ReLU(),\\n        )# out : 8 * 7501\\n        \\n        \\n        \\n    def forward(self, mel_spec):\\n        x = F.pad(mel_spec, pad = (2, 0, 0, 0))\\n        x1 = self.conv1(x) # 48 * 7501 => 512 * 7501 \\n        #print(x1.shape)\\n        \\n        x1to2 = F.pad(x1, pad = (4, 0, 0, 0)) #512 * 7501 \\n        x1to3 = F.pad(x1, pad = (8, 0, 0, 0)) #512 * 7501 \\n        #print('x1to2',x1to2.shape)\\n        #print('x1to3',x1to3.shape)\\n        \\n        \\n        x2 = self.conv2(x1to2) # 512 * 7501 => 256 * 7501\\n        #print(x2.shape)\\n        \\n        x2to3 = F.pad(x2, pad = (8, 0, 0, 0)) # 256 * 7501\\n        x2to4 = F.pad(x2, pad = (16, 0, 0, 0)) # 256 * 7501\\n        #print('x2to3 :',x2to3.shape)\\n        #print('x2to4 :',x2to3.shape)\\n        \\n        x3 = self.conv3(x2to3) # 256 * 7501 => 64 * 7501\\n        x3_connec = self.conv1to3(x1to3) # In : 512 * 7501 , out: 64 * 7501     128\\n        x3= torch.cat([x3,x3_connec],1) # 128 * 7501\\n        #print(x3.shape)\\n        \\n        \\n        x3to4 = F.pad(x3, pad = (16, 0, 0, 0)) # 128 * 7501\\n        x3to5 = F.pad(x3, pad = (32, 0, 0, 0)) # 128 * 7501\\n        #print('x3to4 :',x3to4.shape)\\n        #print('x3to5 :',x3to5.shape)\\n        \\n        x4 = self.conv4(x3to4) # 128 * 7501 => 32 * 7501\\n        x4_connec = self.conv2to4(x2to4) # In : 256 * 7501 , out : 32 * 7501 \\n        x4 = torch.cat([x4,x4_connec],1) # 64 * 7501\\n        #print(x4.shape)\\n        \\n        x4to5 = F.pad(x4, pad = (32, 0, 0, 0)) # 64 * 7501\\n        x4to6 = F.pad(x4, pad = (64, 0, 0, 0)) # 64 * 7501\\n        #print('x4to5 :',x4to5.shape)\\n        #print('x4to6 :',x4to6.shape)\\n        \\n        \\n        x5 = self.conv5(x4to5) # 64 * 7501 = > 16 * 7501 \\n        x5_connec = self.conv3to5(x3to5) # 128 * 7501 => 16 * 7501\\n        x5 = torch.cat([x5,x5_connec],1) # 32 * 7501\\n        #print(x5.shape)\\n        \\n        x5to6 = F.pad(x5, pad = (64, 0, 0, 0)) # 32 * 7501\\n        x5to7 = F.pad(x5, pad = (128, 0, 0, 0)) # 32 * 7501\\n        #print('x5to6 :',x5to6.shape)\\n        #print('x5to7 :',x5to7.shape)\\n        \\n        \\n        x6 = self.conv6(x5to6) # 32 * 7501 => 8 * 7501\\n        x6_connec = self.conv4to6(x4to6) # 64 * 7501 => 8 * 7501\\n        x6 = torch.cat([x6,x6_connec],1) # 16 * 7501\\n        #print(x6.shape)\\n        x6to7 = F.pad(x6, pad = (128, 0, 0, 0)) # 16 * 7501\\n        #print('x6to7 :',x6to7.shape)\\n        \\n        \\n        x7 = self.conv7(x6to7) # 16 * 7501 => 4 *7501 \\n        x7_connec = self.conv5to7(x5to7) # 32 * 7501 => 4 * 7501\\n        x7 = torch.cat([x7,x7_connec],1) # 8 * 7501\\n        #print(x7.shape)\\n        encode = self.encoder_fc(x7.view(-1, 8 * 7501))\\n\\n        #encode = self.encoder_fc(x.view(-1, 8 * 1876))\\n\\n        # print('decode')\\n        x = self.decoder_fc(encode)\\n        x = x.view(-1, 8, 7501)\\n        #x = torch.swapaxes(torch.fliplr(torch.swapaxes(x, 1, 2)), 1, 2)\\n        \\n        x = torch.cat([x7,x],1) # 4,16,7501\\n        #x = self.concat7_shape(x) \\n        x = F.pad(x, pad = (128, 0, 0, 0)) \\n        x = self.t_conv1(x) # 4,8,7501\\n        #print(x.shape,'new_cat7')\\n        \\n        \\n        x = torch.cat([x6,x],1)\\n        #x = self.concat6_shape(x)\\n        x = F.pad(x, pad = (64, 0, 0, 0))\\n        x = self.t_conv2(x)\\n        \\n        \\n        x = torch.cat([x5,x],1)\\n        #x = self.concat5_shape(x)\\n        x = F.pad(x, pad = (32, 0, 0, 0))\\n        x = self.t_conv3(x)\\n        \\n        \\n        \\n        x = torch.cat([x4,x],1)\\n        #x = self.concat4_shape(x)\\n        x = F.pad(x, pad = (16, 0, 0, 0))\\n        x = self.t_conv4(x)\\n        \\n        \\n        x = torch.cat([x3,x],1)\\n        #x = self.concat3_shape(x)\\n        x = F.pad(x, pad = (8, 0, 0, 0))\\n        x = self.t_conv5(x)\\n        \\n        \\n        \\n        x = torch.cat([x2,x],1)\\n        #x = self.concat2_shape(x)\\n        x = F.pad(x, pad = (4, 0, 0, 0))\\n        x = self.t_conv6(x)\\n        \\n        \\n        \\n        x = torch.cat([x1,x],1)\\n        #x = self.concat1_shape(x)\\n        x = F.pad(x, pad = (2, 0, 0, 0))\\n        x = self.t_conv7(x)\\n        \\n        #print(x.shape)\\n        #x = torch.swapaxes(torch.fliplr(torch.swapaxes(x, 1, 2)), 1, 2)\\n        \\n        return encode, x\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# 이거아님\n",
    "############ 원래 ##############\n",
    "class TimeAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TimeAutoEncoder, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 48, out_channels = 512, kernel_size = 3, stride = 1, padding = 0, dilation = 1),\n",
    "            #nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 512, out_channels = 256, kernel_size = 3, stride = 1, padding = 0, dilation = 2),\n",
    "            #nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "            \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 256, out_channels = 64, kernel_size = 3, stride = 1, padding = 0, dilation = 4),\n",
    "            #nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 128, out_channels = 32, kernel_size = 3, stride = 1, padding = 0, dilation = 8),\n",
    "            #nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 64, out_channels = 16, kernel_size = 3, stride = 1, padding = 0, dilation = 16),\n",
    "            #nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv6 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 32, out_channels = 8, kernel_size = 3, stride = 1, padding = 0, dilation = 32),\n",
    "            #nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv7 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 16, out_channels = 4, kernel_size = 3, stride = 1, padding = 0, dilation = 64),\n",
    "            #nn.BatchNorm1d(8),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.encoder_fc = nn.Sequential(\n",
    "            nn.Linear(8 * 7501, 256),\n",
    "            #nn.BatchNorm1d(256),\n",
    "            nn.Tanh(),\n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.decoder_fc = nn.Sequential(\n",
    "            nn.Linear(256, 8 * 7501),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.t_conv1 = nn.Sequential(\n",
    "            # nn.ConvTranspose1d(in_channels = 8, out_channels = 16, kernel_size  = 3, stride = 1, dilation=62),\n",
    "            nn.Conv1d(in_channels = 16, out_channels = 16, kernel_size = 3, stride = 1, padding = 0, dilation = 64),\n",
    "            #nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.t_conv2 = nn.Sequential(\n",
    "            # nn.ConvTranspose1d(in_channels = 16, out_channels = 32, kernel_size  = 3, stride = 1, dilation = 30),\n",
    "            nn.Conv1d(in_channels = 32, out_channels = 32, kernel_size = 3, stride = 1, padding = 0, dilation = 32),\n",
    "            #nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.t_conv3 = nn.Sequential(\n",
    "            # nn.ConvTranspose1d(in_channels = 32, out_channels = 64, kernel_size  = 3, stride = 1, dilation=14),\n",
    "            nn.Conv1d(in_channels = 64, out_channels = 64, kernel_size = 3, stride = 1, padding = 0, dilation = 16),\n",
    "            #nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.t_conv4 = nn.Sequential(\n",
    "            # nn.ConvTranspose1d(in_channels = 64, out_channels = 128, kernel_size  = 3, stride = 1, dilation = 6),\n",
    "            nn.Conv1d(in_channels = 128, out_channels = 128, kernel_size = 3, stride = 1, padding = 0, dilation = 8),\n",
    "            #nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.t_conv5 = nn.Sequential(\n",
    "            # nn.ConvTranspose1d(in_channels = 128, out_channels = 256, kernel_size  = 3, stride = 1, dilation=2),\n",
    "            nn.Conv1d(in_channels = 256, out_channels = 256, kernel_size = 3, stride = 1, padding = 0, dilation = 4),\n",
    "            #nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.t_conv6 = nn.Sequential(\n",
    "            # nn.ConvTranspose1d(in_channels = 256, out_channels = 512, kernel_size  = 3, stride = 1, dilation = 1),\n",
    "            nn.Conv1d(in_channels = 512, out_channels = 512, kernel_size = 3, stride = 1, padding = 0, dilation = 2),\n",
    "            #nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.t_conv7 = nn.Sequential(\n",
    "            # nn.ConvTranspose1d(in_channels = 512, out_channels = 48, kernel_size  = 3, stride = 1, dilation= 1),\n",
    "            nn.Conv1d(in_channels = 1024, out_channels = 48, kernel_size = 3, stride = 1, padding = 0, dilation = 1)\n",
    "        )\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.conv1to3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 512, out_channels = 64, kernel_size = 3, stride = 1, padding = 0, dilation = 4),\n",
    "            #nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "        )# out : 128 * 7501\n",
    "        \n",
    "        self.conv2to4 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 256, out_channels = 32, kernel_size = 3, stride = 1, padding = 0, dilation = 8),\n",
    "            #nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "        )# out : 64 * 7501\n",
    "        \n",
    "        self.conv3to5 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 128, out_channels = 16, kernel_size = 3, stride = 1, padding = 0, dilation = 16),\n",
    "            #nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "        )# out : 32 * 7501\n",
    "        \n",
    "        self.conv4to6 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 64, out_channels = 8, kernel_size = 3, stride = 1, padding = 0, dilation = 32),\n",
    "            #nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "        )# out : 16 * 7501\n",
    "        \n",
    "        self.conv5to7 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 32, out_channels = 4, kernel_size = 3, stride = 1, padding = 0, dilation = 64),\n",
    "            #nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "        )# out : 8 * 7501\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, mel_spec):\n",
    "        x = F.pad(mel_spec, pad = (2, 0, 0, 0))\n",
    "        x1 = self.conv1(x) # 48 * 7501 => 512 * 7501 \n",
    "        #print(x1.shape)\n",
    "        \n",
    "        x1to2 = F.pad(x1, pad = (4, 0, 0, 0)) #512 * 7501 \n",
    "        x1to3 = F.pad(x1, pad = (8, 0, 0, 0)) #512 * 7501 \n",
    "        #print('x1to2',x1to2.shape)\n",
    "        #print('x1to3',x1to3.shape)\n",
    "        \n",
    "        \n",
    "        x2 = self.conv2(x1to2) # 512 * 7501 => 256 * 7501\n",
    "        #print(x2.shape)\n",
    "        \n",
    "        x2to3 = F.pad(x2, pad = (8, 0, 0, 0)) # 256 * 7501\n",
    "        x2to4 = F.pad(x2, pad = (16, 0, 0, 0)) # 256 * 7501\n",
    "        #print('x2to3 :',x2to3.shape)\n",
    "        #print('x2to4 :',x2to3.shape)\n",
    "        \n",
    "        x3 = self.conv3(x2to3) # 256 * 7501 => 64 * 7501\n",
    "        x3_connec = self.conv1to3(x1to3) # In : 512 * 7501 , out: 64 * 7501     128\n",
    "        x3= torch.cat([x3,x3_connec],1) # 128 * 7501\n",
    "        #print(x3.shape)\n",
    "        \n",
    "        \n",
    "        x3to4 = F.pad(x3, pad = (16, 0, 0, 0)) # 128 * 7501\n",
    "        x3to5 = F.pad(x3, pad = (32, 0, 0, 0)) # 128 * 7501\n",
    "        #print('x3to4 :',x3to4.shape)\n",
    "        #print('x3to5 :',x3to5.shape)\n",
    "        \n",
    "        x4 = self.conv4(x3to4) # 128 * 7501 => 32 * 7501\n",
    "        x4_connec = self.conv2to4(x2to4) # In : 256 * 7501 , out : 32 * 7501 \n",
    "        x4 = torch.cat([x4,x4_connec],1) # 64 * 7501\n",
    "        #print(x4.shape)\n",
    "        \n",
    "        x4to5 = F.pad(x4, pad = (32, 0, 0, 0)) # 64 * 7501\n",
    "        x4to6 = F.pad(x4, pad = (64, 0, 0, 0)) # 64 * 7501\n",
    "        #print('x4to5 :',x4to5.shape)\n",
    "        #print('x4to6 :',x4to6.shape)\n",
    "        \n",
    "        \n",
    "        x5 = self.conv5(x4to5) # 64 * 7501 = > 16 * 7501 \n",
    "        x5_connec = self.conv3to5(x3to5) # 128 * 7501 => 16 * 7501\n",
    "        x5 = torch.cat([x5,x5_connec],1) # 32 * 7501\n",
    "        #print(x5.shape)\n",
    "        \n",
    "        x5to6 = F.pad(x5, pad = (64, 0, 0, 0)) # 32 * 7501\n",
    "        x5to7 = F.pad(x5, pad = (128, 0, 0, 0)) # 32 * 7501\n",
    "        #print('x5to6 :',x5to6.shape)\n",
    "        #print('x5to7 :',x5to7.shape)\n",
    "        \n",
    "        \n",
    "        x6 = self.conv6(x5to6) # 32 * 7501 => 8 * 7501\n",
    "        x6_connec = self.conv4to6(x4to6) # 64 * 7501 => 8 * 7501\n",
    "        x6 = torch.cat([x6,x6_connec],1) # 16 * 7501\n",
    "        #print(x6.shape)\n",
    "        x6to7 = F.pad(x6, pad = (128, 0, 0, 0)) # 16 * 7501\n",
    "        #print('x6to7 :',x6to7.shape)\n",
    "        \n",
    "        \n",
    "        x7 = self.conv7(x6to7) # 16 * 7501 => 4 *7501 \n",
    "        x7_connec = self.conv5to7(x5to7) # 32 * 7501 => 4 * 7501\n",
    "        x7 = torch.cat([x7,x7_connec],1) # 8 * 7501\n",
    "        #print(x7.shape)\n",
    "        encode = self.encoder_fc(x7.view(-1, 8 * 7501))\n",
    "\n",
    "        #encode = self.encoder_fc(x.view(-1, 8 * 1876))\n",
    "\n",
    "        # print('decode')\n",
    "        x = self.decoder_fc(encode)\n",
    "        x = x.view(-1, 8, 7501)\n",
    "        #x = torch.swapaxes(torch.fliplr(torch.swapaxes(x, 1, 2)), 1, 2)\n",
    "        \n",
    "        x = torch.cat([x7,x],1) # 4,16,7501\n",
    "        #x = self.concat7_shape(x) \n",
    "        x = F.pad(x, pad = (128, 0, 0, 0)) \n",
    "        x = self.t_conv1(x) # 4,8,7501\n",
    "        #print(x.shape,'new_cat7')\n",
    "        \n",
    "        \n",
    "        x = torch.cat([x6,x],1)\n",
    "        #x = self.concat6_shape(x)\n",
    "        x = F.pad(x, pad = (64, 0, 0, 0))\n",
    "        x = self.t_conv2(x)\n",
    "        \n",
    "        \n",
    "        x = torch.cat([x5,x],1)\n",
    "        #x = self.concat5_shape(x)\n",
    "        x = F.pad(x, pad = (32, 0, 0, 0))\n",
    "        x = self.t_conv3(x)\n",
    "        \n",
    "        \n",
    "        \n",
    "        x = torch.cat([x4,x],1)\n",
    "        #x = self.concat4_shape(x)\n",
    "        x = F.pad(x, pad = (16, 0, 0, 0))\n",
    "        x = self.t_conv4(x)\n",
    "        \n",
    "        \n",
    "        x = torch.cat([x3,x],1)\n",
    "        #x = self.concat3_shape(x)\n",
    "        x = F.pad(x, pad = (8, 0, 0, 0))\n",
    "        x = self.t_conv5(x)\n",
    "        \n",
    "        \n",
    "        \n",
    "        x = torch.cat([x2,x],1)\n",
    "        #x = self.concat2_shape(x)\n",
    "        x = F.pad(x, pad = (4, 0, 0, 0))\n",
    "        x = self.t_conv6(x)\n",
    "        \n",
    "        \n",
    "        \n",
    "        x = torch.cat([x1,x],1)\n",
    "        #x = self.concat1_shape(x)\n",
    "        x = F.pad(x, pad = (2, 0, 0, 0))\n",
    "        x = self.t_conv7(x)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        #x = torch.swapaxes(torch.fliplr(torch.swapaxes(x, 1, 2)), 1, 2)\n",
    "        \n",
    "        return encode, x\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# beta\n",
    "# 뭔가 잘못됐다\n",
    "# swap 지우면 안되나?\n",
    "#저장모델\n",
    "############# 원래 ##############\n",
    "class TimeAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TimeAutoEncoder, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 48, out_channels = 512, kernel_size = 3, stride = 1, padding = 0, dilation = 1),\n",
    "            #nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 512, out_channels = 256, kernel_size = 3, stride = 1, padding = 0, dilation = 2),\n",
    "            #nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "            \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 256, out_channels = 64, kernel_size = 3, stride = 1, padding = 0, dilation = 4),\n",
    "            #nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 128, out_channels = 64, kernel_size = 3, stride = 1, padding = 0, dilation = 8),\n",
    "            #nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 64, out_channels = 16, kernel_size = 3, stride = 1, padding = 0, dilation = 16),\n",
    "            #nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv6 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 32, out_channels = 16, kernel_size = 3, stride = 1, padding = 0, dilation = 32),\n",
    "            #nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv7 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 16, out_channels = 4, kernel_size = 3, stride = 1, padding = 0, dilation = 64),\n",
    "            #nn.BatchNorm1d(8),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.encoder_fc = nn.Sequential(\n",
    "            nn.Linear(8 * 7501, 256),\n",
    "            #nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        \n",
    "        self.decoder_fc = nn.Sequential(\n",
    "            nn.Linear(256, 8 * 7501),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.t_conv1 = nn.Sequential(\n",
    "            # nn.ConvTranspose1d(in_channels = 8, out_channels = 16, kernel_size  = 3, stride = 1, dilation=62),\n",
    "            nn.Conv1d(in_channels = 16, out_channels = 16, kernel_size = 3, stride = 1, padding = 0, dilation = 64),\n",
    "            #nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.t_conv2 = nn.Sequential(\n",
    "            # nn.ConvTranspose1d(in_channels = 16, out_channels = 32, kernel_size  = 3, stride = 1, dilation = 30),\n",
    "            nn.Conv1d(in_channels = 32, out_channels = 32, kernel_size = 3, stride = 1, padding = 0, dilation = 32),\n",
    "            #nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.t_conv3 = nn.Sequential(\n",
    "            # nn.ConvTranspose1d(in_channels = 32, out_channels = 64, kernel_size  = 3, stride = 1, dilation=14),\n",
    "            nn.Conv1d(in_channels = 64, out_channels = 64, kernel_size = 3, stride = 1, padding = 0, dilation = 16),\n",
    "            #nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.t_conv4 = nn.Sequential(\n",
    "            # nn.ConvTranspose1d(in_channels = 64, out_channels = 128, kernel_size  = 3, stride = 1, dilation = 6),\n",
    "            nn.Conv1d(in_channels = 128, out_channels = 128, kernel_size = 3, stride = 1, padding = 0, dilation = 8),\n",
    "            #nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.t_conv5 = nn.Sequential(\n",
    "            # nn.ConvTranspose1d(in_channels = 128, out_channels = 256, kernel_size  = 3, stride = 1, dilation=2),\n",
    "            nn.Conv1d(in_channels = 256, out_channels = 256, kernel_size = 3, stride = 1, padding = 0, dilation = 4),\n",
    "            #nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.t_conv6 = nn.Sequential(\n",
    "            # nn.ConvTranspose1d(in_channels = 256, out_channels = 512, kernel_size  = 3, stride = 1, dilation = 1),\n",
    "            nn.Conv1d(in_channels = 512, out_channels = 512, kernel_size = 3, stride = 1, padding = 0, dilation = 2),\n",
    "            #nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.t_conv7 = nn.Sequential(\n",
    "            # nn.ConvTranspose1d(in_channels = 512, out_channels = 48, kernel_size  = 3, stride = 1, dilation= 1),\n",
    "            nn.Conv1d(in_channels = 1024, out_channels = 48, kernel_size = 3, stride = 1, padding = 0, dilation = 1)\n",
    "        )\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.conv1to3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 512, out_channels = 64, kernel_size = 3, stride = 1, padding = 0, dilation = 4),\n",
    "            #nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "        )# out : 128 * 7501\n",
    "        \n",
    "        self.conv2to4 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 256, out_channels = 32, kernel_size = 3, stride = 1, padding = 0, dilation = 8),\n",
    "            #nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "        )# out : 64 * 7501\n",
    "        \n",
    "        self.conv3to5 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 128, out_channels = 16, kernel_size = 3, stride = 1, padding = 0, dilation = 16),\n",
    "            #nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "        )# out : 32 * 7501\n",
    "        \n",
    "        self.conv4to6 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 64, out_channels = 8, kernel_size = 3, stride = 1, padding = 0, dilation = 32),\n",
    "            #nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "        )# out : 16 * 7501\n",
    "        \n",
    "        self.conv5to7 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 32, out_channels = 4, kernel_size = 3, stride = 1, padding = 0, dilation = 64),\n",
    "            #nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "        )# out : 8 * 7501\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, mel_spec):\n",
    "        x = F.pad(mel_spec, pad = (2, 0, 0, 0))\n",
    "        x1 = self.conv1(x) # 48 * 7501 => 512 * 7501 \n",
    "        #print(x1.shape)\n",
    "        \n",
    "        x1to2 = F.pad(x1, pad = (4, 0, 0, 0)) #512 * 7501 \n",
    "        x1to3 = F.pad(x1, pad = (8, 0, 0, 0)) #512 * 7501 \n",
    "        #print('x1to2',x1to2.shape)\n",
    "        #print('x1to3',x1to3.shape)\n",
    "        \n",
    "        \n",
    "        x2 = self.conv2(x1to2) # 512 * 7501 => 256 * 7501\n",
    "        #print(x2.shape)\n",
    "        \n",
    "        x2to3 = F.pad(x2, pad = (8, 0, 0, 0)) # 256 * 7501\n",
    "        #x2to4 = F.pad(x2, pad = (16, 0, 0, 0)) # 256 * 7501\n",
    "        #print('x2to3 :',x2to3.shape)\n",
    "        #print('x2to4 :',x2to3.shape)\n",
    "        \n",
    "        x3 = self.conv3(x2to3) # 256 * 7501 => 64 * 7501\n",
    "        x3_connec = self.conv1to3(x1to3) # In : 512 * 7501 , out: 64 * 7501     128\n",
    "        x3= torch.cat([x3,x3_connec],1) # 128 * 7501\n",
    "        #print(x3.shape)\n",
    "        \n",
    "        \n",
    "        x3to4 = F.pad(x3, pad = (16, 0, 0, 0)) # 128 * 7501\n",
    "        x3to5 = F.pad(x3, pad = (32, 0, 0, 0)) # 128 * 7501\n",
    "        #print('x3to4 :',x3to4.shape)\n",
    "        #print('x3to5 :',x3to5.shape)\n",
    "        \n",
    "        x4 = self.conv4(x3to4) # 128 * 7501 => 64 * 7501\n",
    "        #x4_connec = self.conv2to4(x2to4) # In : 256 * 7501 , out : 32 * 7501 \n",
    "        #x4 = torch.cat([x4,x4_connec],1) # 64 * 7501\n",
    "        #print(x4.shape)\n",
    "        \n",
    "        x4to5 = F.pad(x4, pad = (32, 0, 0, 0)) # 64 * 7501\n",
    "        #x4to6 = F.pad(x4, pad = (64, 0, 0, 0)) # 64 * 7501\n",
    "        #print('x4to5 :',x4to5.shape)\n",
    "        #print('x4to6 :',x4to6.shape)\n",
    "        \n",
    "        \n",
    "        x5 = self.conv5(x4to5) # 64 * 7501 = > 16 * 7501 \n",
    "        x5_connec = self.conv3to5(x3to5) # 128 * 7501 => 16 * 7501\n",
    "        x5 = torch.cat([x5,x5_connec],1) # 32 * 7501\n",
    "        #print(x5.shape)\n",
    "        \n",
    "        x5to6 = F.pad(x5, pad = (64, 0, 0, 0)) # 32 * 7501\n",
    "        x5to7 = F.pad(x5, pad = (128, 0, 0, 0)) # 32 * 7501\n",
    "        #print('x5to6 :',x5to6.shape)\n",
    "        #print('x5to7 :',x5to7.shape)\n",
    "        \n",
    "        \n",
    "        x6 = self.conv6(x5to6) # 32 * 7501 => 16 * 7501\n",
    "        #x6_connec = self.conv4to6(x4to6) # 64 * 7501 => 8 * 7501\n",
    "        #x6 = torch.cat([x6,x6_connec],1) # 16 * 7501\n",
    "        #print(x6.shape)\n",
    "        x6to7 = F.pad(x6, pad = (128, 0, 0, 0)) # 32 * 7501\n",
    "        #print('x6to7 :',x6to7.shape)\n",
    "        \n",
    "        \n",
    "        x7 = self.conv7(x6to7) # 16 * 7501 => 4 *7501 \n",
    "        pre_encode = torch.flatten(x7)\n",
    "        x7_connec = self.conv5to7(x5to7) # 32 * 7501 => 4 * 7501\n",
    "        x7 = torch.cat([x7,x7_connec],1) # 8 * 7501\n",
    "        #print(x7.shape)\n",
    "        encode = self.encoder_fc(x7.view(-1, 8 * 7501))\n",
    "\n",
    "        #encode = self.encoder_fc(x.view(-1, 8 * 1876))\n",
    "\n",
    "        # print('decode')\n",
    "        x = self.decoder_fc(encode)\n",
    "        x = x.view(-1, 8, 7501)\n",
    "        \n",
    "        #x = torch.swapaxes(torch.fliplr(torch.swapaxes(x, 1, 2)), 1, 2)\n",
    "        \n",
    "        x = torch.cat([x7,x],1) # 4,16,7501\n",
    "        #x = self.concat7_shape(x) \n",
    "        x = F.pad(x, pad = (128, 0, 0, 0)) \n",
    "        x = self.t_conv1(x) # 4,8,7501\n",
    "        #print(x.shape,'new_cat7')\n",
    "        \n",
    "        \n",
    "        x = torch.cat([x6,x],1)\n",
    "        #x = self.concat6_shape(x)\n",
    "        x = F.pad(x, pad = (64, 0, 0, 0))\n",
    "        x = self.t_conv2(x)\n",
    "        \n",
    "        \n",
    "        x = torch.cat([x5,x],1)\n",
    "        #x = self.concat5_shape(x)\n",
    "        x = F.pad(x, pad = (32, 0, 0, 0))\n",
    "        x = self.t_conv3(x)\n",
    "        \n",
    "        \n",
    "        \n",
    "        x = torch.cat([x4,x],1)\n",
    "        #x = self.concat4_shape(x)\n",
    "        x = F.pad(x, pad = (16, 0, 0, 0))\n",
    "        x = self.t_conv4(x)\n",
    "        \n",
    "        \n",
    "        x = torch.cat([x3,x],1)\n",
    "        #x = self.concat3_shape(x)\n",
    "        x = F.pad(x, pad = (8, 0, 0, 0))\n",
    "        x = self.t_conv5(x)\n",
    "        \n",
    "        \n",
    "        \n",
    "        x = torch.cat([x2,x],1)\n",
    "        #x = self.concat2_shape(x)\n",
    "        x = F.pad(x, pad = (4, 0, 0, 0))\n",
    "        x = self.t_conv6(x)\n",
    "        \n",
    "        \n",
    "        \n",
    "        x = torch.cat([x1,x],1)\n",
    "        #x = self.concat1_shape(x)\n",
    "        x = F.pad(x, pad = (2, 0, 0, 0))\n",
    "        x = self.t_conv7(x)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        #x = torch.swapaxes(torch.fliplr(torch.swapaxes(x, 1, 2)), 1, 2)\n",
    "        \n",
    "        return encode, x\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# bravo 저장\n",
    "############# 원래 ##############\n",
    "class TimeAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TimeAutoEncoder, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 48, out_channels = 512, kernel_size = 3, stride = 1, padding = 0, dilation = 1),\n",
    "            #nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 512, out_channels = 256, kernel_size = 3, stride = 1, padding = 0, dilation = 2),\n",
    "            #nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "            \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 256, out_channels = 64, kernel_size = 3, stride = 1, padding = 0, dilation = 4),\n",
    "            #nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 128, out_channels = 64, kernel_size = 3, stride = 1, padding = 0, dilation = 8),\n",
    "            #nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 64, out_channels = 16, kernel_size = 3, stride = 1, padding = 0, dilation = 16),\n",
    "            #nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv6 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 32, out_channels = 16, kernel_size = 3, stride = 1, padding = 0, dilation = 32),\n",
    "            #nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv7 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 16, out_channels = 4, kernel_size = 3, stride = 1, padding = 0, dilation = 64),\n",
    "            #nn.BatchNorm1d(8),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.encoder_fc = nn.Sequential(\n",
    "            nn.Linear(8 * 7501, 256),\n",
    "            #nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        \n",
    "        self.decoder_fc = nn.Sequential(\n",
    "            nn.Linear(256, 8 * 7501),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.t_conv1 = nn.Sequential(\n",
    "            # nn.ConvTranspose1d(in_channels = 8, out_channels = 16, kernel_size  = 3, stride = 1, dilation=62),\n",
    "            nn.Conv1d(in_channels = 16, out_channels = 16, kernel_size = 3, stride = 1, padding = 0, dilation = 64),\n",
    "            #nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.t_conv2 = nn.Sequential(\n",
    "            # nn.ConvTranspose1d(in_channels = 16, out_channels = 32, kernel_size  = 3, stride = 1, dilation = 30),\n",
    "            nn.Conv1d(in_channels = 32, out_channels = 32, kernel_size = 3, stride = 1, padding = 0, dilation = 32),\n",
    "            #nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.t_conv3 = nn.Sequential(\n",
    "            # nn.ConvTranspose1d(in_channels = 32, out_channels = 64, kernel_size  = 3, stride = 1, dilation=14),\n",
    "            nn.Conv1d(in_channels = 64, out_channels = 64, kernel_size = 3, stride = 1, padding = 0, dilation = 16),\n",
    "            #nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.t_conv4 = nn.Sequential(\n",
    "            # nn.ConvTranspose1d(in_channels = 64, out_channels = 128, kernel_size  = 3, stride = 1, dilation = 6),\n",
    "            nn.Conv1d(in_channels = 128, out_channels = 128, kernel_size = 3, stride = 1, padding = 0, dilation = 8),\n",
    "            #nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.t_conv5 = nn.Sequential(\n",
    "            # nn.ConvTranspose1d(in_channels = 128, out_channels = 256, kernel_size  = 3, stride = 1, dilation=2),\n",
    "            nn.Conv1d(in_channels = 256, out_channels = 256, kernel_size = 3, stride = 1, padding = 0, dilation = 4),\n",
    "            #nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.t_conv6 = nn.Sequential(\n",
    "            # nn.ConvTranspose1d(in_channels = 256, out_channels = 512, kernel_size  = 3, stride = 1, dilation = 1),\n",
    "            nn.Conv1d(in_channels = 512, out_channels = 512, kernel_size = 3, stride = 1, padding = 0, dilation = 2),\n",
    "            #nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.t_conv7 = nn.Sequential(\n",
    "            # nn.ConvTranspose1d(in_channels = 512, out_channels = 48, kernel_size  = 3, stride = 1, dilation= 1),\n",
    "            nn.Conv1d(in_channels = 1024, out_channels = 48, kernel_size = 3, stride = 1, padding = 0, dilation = 1)\n",
    "        )\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.conv1to3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 512, out_channels = 64, kernel_size = 3, stride = 1, padding = 0, dilation = 4),\n",
    "            #nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "        )# out : 128 * 7501\n",
    "        \n",
    "        self.conv2to4 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 256, out_channels = 32, kernel_size = 3, stride = 1, padding = 0, dilation = 8),\n",
    "            #nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "        )# out : 64 * 7501\n",
    "        \n",
    "        self.conv3to5 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 128, out_channels = 16, kernel_size = 3, stride = 1, padding = 0, dilation = 16),\n",
    "            #nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "        )# out : 32 * 7501\n",
    "        \n",
    "        self.conv4to6 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 64, out_channels = 8, kernel_size = 3, stride = 1, padding = 0, dilation = 32),\n",
    "            #nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "        )# out : 16 * 7501\n",
    "        \n",
    "        self.conv5to7 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 32, out_channels = 4, kernel_size = 3, stride = 1, padding = 0, dilation = 64),\n",
    "            #nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "        )# out : 8 * 7501\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, mel_spec):\n",
    "        x = F.pad(mel_spec, pad = (2, 0, 0, 0))\n",
    "        x1 = self.conv1(x) # 48 * 7501 => 512 * 7501 \n",
    "        #print(x1.shape)\n",
    "        \n",
    "        x1to2 = F.pad(x1, pad = (4, 0, 0, 0)) #512 * 7501 \n",
    "        x1to3 = F.pad(x1, pad = (8, 0, 0, 0)) #512 * 7501 \n",
    "        #print('x1to2',x1to2.shape)\n",
    "        #print('x1to3',x1to3.shape)\n",
    "        \n",
    "        \n",
    "        x2 = self.conv2(x1to2) # 512 * 7501 => 256 * 7501\n",
    "        #print(x2.shape)\n",
    "        \n",
    "        x2to3 = F.pad(x2, pad = (8, 0, 0, 0)) # 256 * 7501\n",
    "        #x2to4 = F.pad(x2, pad = (16, 0, 0, 0)) # 256 * 7501\n",
    "        #print('x2to3 :',x2to3.shape)\n",
    "        #print('x2to4 :',x2to3.shape)\n",
    "        \n",
    "        x3 = self.conv3(x2to3) # 256 * 7501 => 64 * 7501\n",
    "        x3_connec = self.conv1to3(x1to3) # In : 512 * 7501 , out: 64 * 7501     128\n",
    "        x3= torch.cat([x3,x3_connec],1) # 128 * 7501\n",
    "        #print(x3.shape)\n",
    "        \n",
    "        \n",
    "        x3to4 = F.pad(x3, pad = (16, 0, 0, 0)) # 128 * 7501\n",
    "        x3to5 = F.pad(x3, pad = (32, 0, 0, 0)) # 128 * 7501\n",
    "        #print('x3to4 :',x3to4.shape)\n",
    "        #print('x3to5 :',x3to5.shape)\n",
    "        \n",
    "        x4 = self.conv4(x3to4) # 128 * 7501 => 64 * 7501\n",
    "        #x4_connec = self.conv2to4(x2to4) # In : 256 * 7501 , out : 32 * 7501 \n",
    "        #x4 = torch.cat([x4,x4_connec],1) # 64 * 7501\n",
    "        #print(x4.shape)\n",
    "        \n",
    "        x4to5 = F.pad(x4, pad = (32, 0, 0, 0)) # 64 * 7501\n",
    "        #x4to6 = F.pad(x4, pad = (64, 0, 0, 0)) # 64 * 7501\n",
    "        #print('x4to5 :',x4to5.shape)\n",
    "        #print('x4to6 :',x4to6.shape)\n",
    "        \n",
    "        \n",
    "        x5 = self.conv5(x4to5) # 64 * 7501 = > 16 * 7501 \n",
    "        x5_connec = self.conv3to5(x3to5) # 128 * 7501 => 16 * 7501\n",
    "        x5 = torch.cat([x5,x5_connec],1) # 32 * 7501\n",
    "        #print(x5.shape)\n",
    "        \n",
    "        x5to6 = F.pad(x5, pad = (64, 0, 0, 0)) # 32 * 7501\n",
    "        x5to7 = F.pad(x5, pad = (128, 0, 0, 0)) # 32 * 7501\n",
    "        #print('x5to6 :',x5to6.shape)\n",
    "        #print('x5to7 :',x5to7.shape)\n",
    "        \n",
    "        \n",
    "        x6 = self.conv6(x5to6) # 32 * 7501 => 16 * 7501\n",
    "        #x6_connec = self.conv4to6(x4to6) # 64 * 7501 => 8 * 7501\n",
    "        #x6 = torch.cat([x6,x6_connec],1) # 16 * 7501\n",
    "        #print(x6.shape)\n",
    "        x6to7 = F.pad(x6, pad = (128, 0, 0, 0)) # 32 * 7501\n",
    "        #print('x6to7 :',x6to7.shape)\n",
    "        \n",
    "        \n",
    "        x7 = self.conv7(x6to7) # 16 * 7501 => 4 *7501 \n",
    "        pre_encode = torch.flatten(x7)\n",
    "        x7_connec = self.conv5to7(x5to7) # 32 * 7501 => 4 * 7501\n",
    "        x7 = torch.cat([x7,x7_connec],1) # 8 * 7501\n",
    "        #print(x7.shape)\n",
    "        encode = self.encoder_fc(x7.view(-1, 8 * 7501))\n",
    "\n",
    "        #encode = self.encoder_fc(x.view(-1, 8 * 1876))\n",
    "\n",
    "        # print('decode')\n",
    "        x = self.decoder_fc(encode)\n",
    "        x = x.view(-1, 8, 7501)\n",
    "        \n",
    "        #x = torch.swapaxes(torch.fliplr(torch.swapaxes(x, 1, 2)), 1, 2)\n",
    "        \n",
    "        x = torch.cat([x7,x],1) # 4,16,7501\n",
    "        #x = self.concat7_shape(x) \n",
    "        x = F.pad(x, pad = (128, 0, 0, 0)) \n",
    "        x = self.t_conv1(x) # 4,8,7501\n",
    "        #print(x.shape,'new_cat7')\n",
    "        \n",
    "        \n",
    "        x = torch.cat([x6,x],1)\n",
    "        #x = self.concat6_shape(x)\n",
    "        x = F.pad(x, pad = (64, 0, 0, 0))\n",
    "        x = self.t_conv2(x)\n",
    "        \n",
    "        \n",
    "        x = torch.cat([x5,x],1)\n",
    "        #x = self.concat5_shape(x)\n",
    "        x = F.pad(x, pad = (32, 0, 0, 0))\n",
    "        x = self.t_conv3(x)\n",
    "        \n",
    "        \n",
    "        \n",
    "        x = torch.cat([x4,x],1)\n",
    "        #x = self.concat4_shape(x)\n",
    "        x = F.pad(x, pad = (16, 0, 0, 0))\n",
    "        x = self.t_conv4(x)\n",
    "        \n",
    "        \n",
    "        x = torch.cat([x3,x],1)\n",
    "        #x = self.concat3_shape(x)\n",
    "        x = F.pad(x, pad = (8, 0, 0, 0))\n",
    "        x = self.t_conv5(x)\n",
    "        \n",
    "        \n",
    "        \n",
    "        x = torch.cat([x2,x],1)\n",
    "        #x = self.concat2_shape(x)\n",
    "        x = F.pad(x, pad = (4, 0, 0, 0))\n",
    "        x = self.t_conv6(x)\n",
    "        \n",
    "        \n",
    "        \n",
    "        x = torch.cat([x1,x],1)\n",
    "        #x = self.concat1_shape(x)\n",
    "        x = F.pad(x, pad = (2, 0, 0, 0))\n",
    "        x = self.t_conv7(x)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        #x = torch.swapaxes(torch.fliplr(torch.swapaxes(x, 1, 2)), 1, 2)\n",
    "        \n",
    "        return encode, x\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#bravo 노저장\n",
    "############# 원래 ##############\n",
    "class TimeAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TimeAutoEncoder, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 48, out_channels = 128, kernel_size = 3, stride = 1, padding = 0, dilation = 1),\n",
    "            #nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 128, out_channels = 64, kernel_size = 3, stride = 1, padding = 0, dilation = 2),\n",
    "            #nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "            \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 64, out_channels = 32, kernel_size = 3, stride = 1, padding = 0, dilation = 4),\n",
    "            #nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 32, out_channels = 16, kernel_size = 3, stride = 1, padding = 0, dilation = 8),\n",
    "            #nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        \n",
    "        self.encoder_fc = nn.Sequential(\n",
    "            nn.Linear(16 * 7501, 1024),\n",
    "            nn.LeakyReLU(),\n",
    "            #nn.Linear(1024,256),\n",
    "            #nn.BatchNorm1d(128),\n",
    "            #nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.decoder_fc = nn.Sequential(\n",
    "            #nn.Linear(256, 1024),\n",
    "            #nn.ReLU(),\n",
    "            nn.Linear(1024, 16 * 7501),\n",
    "            nn.ReLU(),\n",
    "        \n",
    "        )\n",
    "\n",
    "        self.t_conv1 = nn.Sequential(\n",
    "            # nn.ConvTranspose1d(in_channels = 8, out_channels = 16, kernel_size  = 3, stride = 1, dilation=62),\n",
    "            nn.Conv1d(in_channels = 16, out_channels = 16, kernel_size = 3, stride = 1, padding = 0, dilation = 8),\n",
    "            #nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.t_conv2 = nn.Sequential(\n",
    "            # nn.ConvTranspose1d(in_channels = 16, out_channels = 32, kernel_size  = 3, stride = 1, dilation = 30),\n",
    "            nn.Conv1d(in_channels = 32, out_channels = 32, kernel_size = 3, stride = 1, padding = 0, dilation = 4),\n",
    "            #nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.t_conv3 = nn.Sequential(\n",
    "            # nn.ConvTranspose1d(in_channels = 32, out_channels = 64, kernel_size  = 3, stride = 1, dilation=14),\n",
    "            nn.Conv1d(in_channels = 64, out_channels = 64, kernel_size = 3, stride = 1, padding = 0, dilation = 2),\n",
    "            #nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.t_conv4 = nn.Sequential(\n",
    "            # nn.ConvTranspose1d(in_channels = 64, out_channels = 128, kernel_size  = 3, stride = 1, dilation = 6),\n",
    "            nn.Conv1d(in_channels = 128, out_channels = 48, kernel_size = 3, stride = 1, padding = 0, dilation = 1),\n",
    "            #nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, mel_spec):\n",
    "        x = F.pad(mel_spec, pad = (2, 0, 0, 0))\n",
    "        \n",
    "        x1 = self.conv1(x) # 48 * 7501 => 128 * 7501 \n",
    "        \n",
    "        x1to2 = F.pad(x1, pad = (4, 0, 0, 0)) #512 * 7501 \n",
    "        \n",
    "        x2 = self.conv2(x1to2) # 128 * 7501 => 64 * 7501\n",
    "        \n",
    "        x2to3 = F.pad(x2, pad = (8, 0, 0, 0)) # 64 * 7501\n",
    "        \n",
    "        x3 = self.conv3(x2to3) # 64 * 7501 => 32 * 7501\n",
    "        \n",
    "        x3to4 = F.pad(x3, pad = (16, 0, 0, 0)) # 32 * 7501\n",
    "        \n",
    "        x4 = self.conv4(x3to4) # 32 * 7501 => 16 * 7501\n",
    "        \n",
    "        encode = self.encoder_fc(x4.view(-1, 16 * 7501))\n",
    "\n",
    "        \n",
    "        # print('decode')\n",
    "        x = self.decoder_fc(encode)\n",
    "        t_x0 = x.view(-1, 16, 7501)\n",
    "        \n",
    "        x = self.t_conv1(t_x0) # in: 16 * 7501 , out : 16 * 7485\n",
    "        x = F.pad(x, pad = (16, 0, 0, 0)) # 16 * 7501\n",
    "        t_x1 = torch.cat([x4,x],1) # 32 * 7501\n",
    "        \n",
    "      \n",
    "        x = self.t_conv2(t_x1) # in : 32 * 7501 , out : 32 *  \n",
    "        x = F.pad(x, pad = (8, 0, 0, 0)) \n",
    "        t_x2 = torch.cat([x3,x],1)\n",
    "        \n",
    "        \n",
    "        x = self.t_conv3(t_x2) # 64 * 7501\n",
    "        x = F.pad(x, pad = (4, 0, 0, 0))\n",
    "        t_x3 = torch.cat([x2,x],1) # 128 * 7501\n",
    "        \n",
    "        \n",
    "        x = F.pad(t_x3, pad = (2, 0, 0, 0))\n",
    "        x = self.t_conv4(x) \n",
    "        \n",
    "        return encode, x\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#charlie\n",
    "############# 원래 ##############\n",
    "class TimeAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TimeAutoEncoder, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 48, out_channels = 512, kernel_size = 3, stride = 1, padding = 0, dilation = 1),\n",
    "            #nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 512, out_channels = 256, kernel_size = 3, stride = 1, padding = 0, dilation = 2),\n",
    "            #nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "            \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 256, out_channels = 64, kernel_size = 3, stride = 1, padding = 0, dilation = 4),\n",
    "            #nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 128, out_channels = 64, kernel_size = 3, stride = 1, padding = 0, dilation = 8),\n",
    "            #nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 64, out_channels = 16, kernel_size = 3, stride = 1, padding = 0, dilation = 16),\n",
    "            #nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv6 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 32, out_channels = 16, kernel_size = 3, stride = 1, padding = 0, dilation = 32),\n",
    "            #nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv7 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 16, out_channels = 4, kernel_size = 3, stride = 1, padding = 0, dilation = 64),\n",
    "            #nn.BatchNorm1d(8),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.encoder_fc = nn.Sequential(\n",
    "            nn.Linear(8 * 7501, 256),\n",
    "            #nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        \n",
    "        self.decoder_fc = nn.Sequential(\n",
    "            nn.Linear(256, 8 * 7501),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.t_conv1 = nn.Sequential(\n",
    "            # nn.ConvTranspose1d(in_channels = 8, out_channels = 16, kernel_size  = 3, stride = 1, dilation=62),\n",
    "            nn.Conv1d(in_channels = 16, out_channels = 16, kernel_size = 3, stride = 1, padding = 0, dilation = 1),\n",
    "            #nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.t_conv2 = nn.Sequential(\n",
    "            # nn.ConvTranspose1d(in_channels = 16, out_channels = 32, kernel_size  = 3, stride = 1, dilation = 30),\n",
    "            nn.Conv1d(in_channels = 32, out_channels = 32, kernel_size = 3, stride = 1, padding = 0, dilation = 2),\n",
    "            #nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.t_conv3 = nn.Sequential(\n",
    "            # nn.ConvTranspose1d(in_channels = 32, out_channels = 64, kernel_size  = 3, stride = 1, dilation=14),\n",
    "            nn.Conv1d(in_channels = 64, out_channels = 64, kernel_size = 3, stride = 1, padding = 0, dilation = 4),\n",
    "            #nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.t_conv4 = nn.Sequential(\n",
    "            # nn.ConvTranspose1d(in_channels = 64, out_channels = 128, kernel_size  = 3, stride = 1, dilation = 6),\n",
    "            nn.Conv1d(in_channels = 128, out_channels = 128, kernel_size = 3, stride = 1, padding = 0, dilation = 8),\n",
    "            #nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.t_conv5 = nn.Sequential(\n",
    "            # nn.ConvTranspose1d(in_channels = 128, out_channels = 256, kernel_size  = 3, stride = 1, dilation=2),\n",
    "            nn.Conv1d(in_channels = 256, out_channels = 256, kernel_size = 3, stride = 1, padding = 0, dilation = 16),\n",
    "            #nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.t_conv6 = nn.Sequential(\n",
    "            # nn.ConvTranspose1d(in_channels = 256, out_channels = 512, kernel_size  = 3, stride = 1, dilation = 1),\n",
    "            nn.Conv1d(in_channels = 512, out_channels = 512, kernel_size = 3, stride = 1, padding = 0, dilation = 32),\n",
    "            #nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.t_conv7 = nn.Sequential(\n",
    "            # nn.ConvTranspose1d(in_channels = 512, out_channels = 48, kernel_size  = 3, stride = 1, dilation= 1),\n",
    "            nn.Conv1d(in_channels = 1024, out_channels = 48, kernel_size = 3, stride = 1, padding = 0, dilation = 64)\n",
    "        )\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.conv1to3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 512, out_channels = 64, kernel_size = 3, stride = 1, padding = 0, dilation = 4),\n",
    "            #nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "        )# out : 128 * 7501\n",
    "        \n",
    "        self.conv2to4 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 256, out_channels = 32, kernel_size = 3, stride = 1, padding = 0, dilation = 8),\n",
    "            #nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "        )# out : 64 * 7501\n",
    "        \n",
    "        self.conv3to5 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 128, out_channels = 16, kernel_size = 3, stride = 1, padding = 0, dilation = 16),\n",
    "            #nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "        )# out : 32 * 7501\n",
    "        \n",
    "        self.conv4to6 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 64, out_channels = 8, kernel_size = 3, stride = 1, padding = 0, dilation = 32),\n",
    "            #nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "        )# out : 16 * 7501\n",
    "        \n",
    "        self.conv5to7 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 32, out_channels = 4, kernel_size = 3, stride = 1, padding = 0, dilation = 64),\n",
    "            #nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "        )# out : 8 * 7501\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, mel_spec):\n",
    "        x = F.pad(mel_spec, pad = (2, 0, 0, 0))\n",
    "        x1 = self.conv1(x) # 48 * 7501 => 512 * 7501 \n",
    "        #print(x1.shape)\n",
    "        \n",
    "        x1to2 = F.pad(x1, pad = (4, 0, 0, 0)) #512 * 7501 \n",
    "        x1to3 = F.pad(x1, pad = (8, 0, 0, 0)) #512 * 7501 \n",
    "        #print('x1to2',x1to2.shape)\n",
    "        #print('x1to3',x1to3.shape)\n",
    "        \n",
    "        \n",
    "        x2 = self.conv2(x1to2) # 512 * 7501 => 256 * 7501\n",
    "        #print(x2.shape)\n",
    "        \n",
    "        x2to3 = F.pad(x2, pad = (8, 0, 0, 0)) # 256 * 7501\n",
    "        #x2to4 = F.pad(x2, pad = (16, 0, 0, 0)) # 256 * 7501\n",
    "        #print('x2to3 :',x2to3.shape)\n",
    "        #print('x2to4 :',x2to3.shape)\n",
    "        \n",
    "        x3 = self.conv3(x2to3) # 256 * 7501 => 64 * 7501\n",
    "        x3_connec = self.conv1to3(x1to3) # In : 512 * 7501 , out: 64 * 7501     128\n",
    "        x3= torch.cat([x3,x3_connec],1) # 128 * 7501\n",
    "        #print(x3.shape)\n",
    "        \n",
    "        \n",
    "        x3to4 = F.pad(x3, pad = (16, 0, 0, 0)) # 128 * 7501\n",
    "        x3to5 = F.pad(x3, pad = (32, 0, 0, 0)) # 128 * 7501\n",
    "        #print('x3to4 :',x3to4.shape)\n",
    "        #print('x3to5 :',x3to5.shape)\n",
    "        \n",
    "        x4 = self.conv4(x3to4) # 128 * 7501 => 64 * 7501\n",
    "        #x4_connec = self.conv2to4(x2to4) # In : 256 * 7501 , out : 32 * 7501 \n",
    "        #x4 = torch.cat([x4,x4_connec],1) # 64 * 7501\n",
    "        #print(x4.shape)\n",
    "        \n",
    "        x4to5 = F.pad(x4, pad = (32, 0, 0, 0)) # 64 * 7501\n",
    "        #x4to6 = F.pad(x4, pad = (64, 0, 0, 0)) # 64 * 7501\n",
    "        #print('x4to5 :',x4to5.shape)\n",
    "        #print('x4to6 :',x4to6.shape)\n",
    "        \n",
    "        \n",
    "        x5 = self.conv5(x4to5) # 64 * 7501 = > 16 * 7501 \n",
    "        x5_connec = self.conv3to5(x3to5) # 128 * 7501 => 16 * 7501\n",
    "        x5 = torch.cat([x5,x5_connec],1) # 32 * 7501\n",
    "        #print(x5.shape)\n",
    "        \n",
    "        x5to6 = F.pad(x5, pad = (64, 0, 0, 0)) # 32 * 7501\n",
    "        x5to7 = F.pad(x5, pad = (128, 0, 0, 0)) # 32 * 7501\n",
    "        #print('x5to6 :',x5to6.shape)\n",
    "        #print('x5to7 :',x5to7.shape)\n",
    "        \n",
    "        \n",
    "        x6 = self.conv6(x5to6) # 32 * 7501 => 16 * 7501\n",
    "        #x6_connec = self.conv4to6(x4to6) # 64 * 7501 => 8 * 7501\n",
    "        #x6 = torch.cat([x6,x6_connec],1) # 16 * 7501\n",
    "        #print(x6.shape)\n",
    "        x6to7 = F.pad(x6, pad = (128, 0, 0, 0)) # 32 * 7501\n",
    "        #print('x6to7 :',x6to7.shape)\n",
    "        \n",
    "        \n",
    "        x7 = self.conv7(x6to7) # 16 * 7501 => 4 *7501 \n",
    "        pre_encode = torch.flatten(x7)\n",
    "        x7_connec = self.conv5to7(x5to7) # 32 * 7501 => 4 * 7501\n",
    "        x7 = torch.cat([x7,x7_connec],1) # 8 * 7501\n",
    "        #print(x7.shape)\n",
    "        encode = self.encoder_fc(x7.view(-1, 8 * 7501))\n",
    "\n",
    "        #encode = self.encoder_fc(x.view(-1, 8 * 1876))\n",
    "\n",
    "        # print('decode')\n",
    "        x = self.decoder_fc(encode)\n",
    "        x = x.view(-1, 8, 7501)\n",
    "        \n",
    "        #x = torch.swapaxes(torch.fliplr(torch.swapaxes(x, 1, 2)), 1, 2)\n",
    "        \n",
    "        x = torch.cat([x7,x],1) # 4,16,7501\n",
    "        #x = self.concat7_shape(x) \n",
    "        x = F.pad(x, pad = (2, 0, 0, 0)) \n",
    "        x = self.t_conv1(x) # 4,8,7501\n",
    "        #print(x.shape,'new_cat7')\n",
    "        \n",
    "        \n",
    "        x = torch.cat([x6,x],1)\n",
    "        #x = self.concat6_shape(x)\n",
    "        x = F.pad(x, pad = (4, 0, 0, 0))\n",
    "        x = self.t_conv2(x)\n",
    "        \n",
    "        \n",
    "        x = torch.cat([x5,x],1)\n",
    "        #x = self.concat5_shape(x)\n",
    "        x = F.pad(x, pad = (8, 0, 0, 0))\n",
    "        x = self.t_conv3(x)\n",
    "        \n",
    "        \n",
    "        \n",
    "        x = torch.cat([x4,x],1)\n",
    "        #x = self.concat4_shape(x)\n",
    "        x = F.pad(x, pad = (16, 0, 0, 0))\n",
    "        x = self.t_conv4(x)\n",
    "        \n",
    "        \n",
    "        x = torch.cat([x3,x],1)\n",
    "        #x = self.concat3_shape(x)\n",
    "        x = F.pad(x, pad = (32, 0, 0, 0))\n",
    "        x = self.t_conv5(x)\n",
    "        \n",
    "        \n",
    "        \n",
    "        x = torch.cat([x2,x],1)\n",
    "        #x = self.concat2_shape(x)\n",
    "        x = F.pad(x, pad = (64, 0, 0, 0))\n",
    "        x = self.t_conv6(x)\n",
    "        \n",
    "        \n",
    "        \n",
    "        x = torch.cat([x1,x],1)\n",
    "        #x = self.concat1_shape(x)\n",
    "        x = F.pad(x, pad = (128, 0, 0, 0))\n",
    "        x = self.t_conv7(x)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        #x = torch.swapaxes(torch.fliplr(torch.swapaxes(x, 1, 2)), 1, 2)\n",
    "        \n",
    "        return encode, x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TimeAutoEncoder().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fc1b8baa9cb4930afd2aa8cdfad4b93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH:1, Train Loss:110260.32459309897, Val Loss:11433.421253551136, 학습 시간: 78.66790819168091\n",
      "EPOCH:2, Train Loss:7394.33687133789, Val Loss:3800.166681463068, 학습 시간: 71.63565707206726\n",
      "EPOCH:3, Train Loss:2928.3070871988934, Val Loss:1872.6620871803977, 학습 시간: 57.03754138946533\n",
      "EPOCH:4, Train Loss:1687.3794443766276, Val Loss:1260.5794039639559, 학습 시간: 45.16006374359131\n",
      "EPOCH:5, Train Loss:1121.6509796142577, Val Loss:912.6757535067471, 학습 시간: 36.65596675872803\n",
      "EPOCH:6, Train Loss:864.2644838968913, Val Loss:726.4970619895241, 학습 시간: 35.88951921463013\n",
      "EPOCH:7, Train Loss:682.7494117736817, Val Loss:561.0816653858532, 학습 시간: 35.71298885345459\n",
      "EPOCH:8, Train Loss:554.2170783996582, Val Loss:519.9623315984553, 학습 시간: 35.77355408668518\n",
      "EPOCH:9, Train Loss:607.3109842936198, Val Loss:447.49970176003194, 학습 시간: 35.61007761955261\n",
      "EPOCH:10, Train Loss:477.72823333740234, Val Loss:532.4633622602983, 학습 시간: 35.18342614173889\n",
      "EPOCH:11, Train Loss:401.6184939066569, Val Loss:347.6201789162376, 학습 시간: 34.695720195770264\n",
      "EPOCH:12, Train Loss:323.973624420166, Val Loss:308.0288585316051, 학습 시간: 35.60895609855652\n",
      "EPOCH:13, Train Loss:290.100657526652, Val Loss:268.275034124201, 학습 시간: 35.84679388999939\n",
      "EPOCH:14, Train Loss:313.8174774169922, Val Loss:265.9197006225586, 학습 시간: 35.68574380874634\n",
      "EPOCH:15, Train Loss:238.78607082366943, Val Loss:214.54939686168325, 학습 시간: 35.341413736343384\n",
      "EPOCH:16, Train Loss:229.17393709818523, Val Loss:194.60092475197533, 학습 시간: 35.22715616226196\n",
      "EPOCH:17, Train Loss:194.05967032114665, Val Loss:182.22713713212445, 학습 시간: 34.34017610549927\n",
      "EPOCH:18, Train Loss:229.26340300242106, Val Loss:461.4044203324751, 학습 시간: 35.056241035461426\n",
      "EPOCH:19, Train Loss:217.9830738067627, Val Loss:158.16957196322355, 학습 시간: 35.86855888366699\n",
      "EPOCH:20, Train Loss:161.56976903279622, Val Loss:199.67559363625267, 학습 시간: 34.80478549003601\n",
      "EPOCH:21, Train Loss:217.84127254486083, Val Loss:142.13414348255503, 학습 시간: 33.52404570579529\n",
      "EPOCH:22, Train Loss:145.91367403666177, Val Loss:225.6616301103072, 학습 시간: 35.475128173828125\n",
      "EPOCH:23, Train Loss:143.94332962036134, Val Loss:147.8850080316717, 학습 시간: 35.30222678184509\n",
      "EPOCH:24, Train Loss:104.71568075815837, Val Loss:91.80353580821644, 학습 시간: 35.01532983779907\n",
      "EPOCH:25, Train Loss:105.31534458796183, Val Loss:96.16325742548162, 학습 시간: 35.11035370826721\n",
      "EPOCH:26, Train Loss:151.86355311075846, Val Loss:78.02237606048584, 학습 시간: 34.582892656326294\n",
      "EPOCH:27, Train Loss:72.53471425374349, Val Loss:89.16891444813122, 학습 시간: 35.20557928085327\n",
      "EPOCH:28, Train Loss:99.48459803263346, Val Loss:95.62714906172319, 학습 시간: 35.31842350959778\n",
      "EPOCH:29, Train Loss:92.86581106185913, Val Loss:150.05311480435458, 학습 시간: 35.30153298377991\n",
      "EPOCH:30, Train Loss:107.90639263788859, Val Loss:56.70880265669389, 학습 시간: 34.85111856460571\n",
      "EPOCH:31, Train Loss:67.97554063796997, Val Loss:54.464982292868875, 학습 시간: 34.79559564590454\n",
      "EPOCH:32, Train Loss:61.29490138689677, Val Loss:81.7488776770505, 학습 시간: 35.44403886795044\n",
      "EPOCH:33, Train Loss:63.15171397527059, Val Loss:79.48076872392134, 학습 시간: 35.485153913497925\n",
      "EPOCH:34, Train Loss:67.65611397425333, Val Loss:46.96646777066317, 학습 시간: 35.68626403808594\n",
      "EPOCH:35, Train Loss:48.7513721148173, Val Loss:77.22865659540349, 학습 시간: 35.22290349006653\n",
      "EPOCH:36, Train Loss:68.53959353764851, Val Loss:46.54348364743319, 학습 시간: 34.90697526931763\n",
      "EPOCH:37, Train Loss:43.01157027880351, Val Loss:35.866590239784934, 학습 시간: 34.767130851745605\n",
      "EPOCH:38, Train Loss:68.68327320416769, Val Loss:188.9869596307928, 학습 시간: 35.58491826057434\n",
      "EPOCH:39, Train Loss:143.82795464197795, Val Loss:101.28138264742765, 학습 시간: 35.49625611305237\n",
      "EPOCH:40, Train Loss:66.80655339558919, Val Loss:60.991954283280805, 학습 시간: 35.354564905166626\n",
      "EPOCH:41, Train Loss:46.305172522862755, Val Loss:98.40654251792215, 학습 시간: 35.531562089920044\n",
      "EPOCH:42, Train Loss:57.04865969022115, Val Loss:25.830465230074797, 학습 시간: 35.305041790008545\n",
      "EPOCH:43, Train Loss:32.0077833255132, Val Loss:23.406921776858244, 학습 시간: 34.420860290527344\n",
      "EPOCH:44, Train Loss:38.80158710479736, Val Loss:74.11419486999512, 학습 시간: 35.712918758392334\n",
      "EPOCH:45, Train Loss:45.1844082514445, Val Loss:25.38695391741666, 학습 시간: 35.616586685180664\n",
      "EPOCH:46, Train Loss:44.60436445871989, Val Loss:25.08222198486328, 학습 시간: 34.97810626029968\n",
      "EPOCH:47, Train Loss:45.21771394411723, Val Loss:24.6423414403742, 학습 시간: 34.853801012039185\n",
      "EPOCH:48, Train Loss:27.32829616069794, Val Loss:53.6351295817982, 학습 시간: 35.352527379989624\n",
      "EPOCH:49, Train Loss:34.22537879149119, Val Loss:22.78279506076466, 학습 시간: 35.77957463264465\n",
      "EPOCH:50, Train Loss:23.02162514527639, Val Loss:59.42874977805398, 학습 시간: 35.4737982749939\n",
      "EPOCH:51, Train Loss:37.06577712694804, Val Loss:38.73234558105469, 학습 시간: 35.67736101150513\n",
      "EPOCH:52, Train Loss:33.9316548426946, Val Loss:29.670434431596235, 학습 시간: 35.03521752357483\n",
      "EPOCH:53, Train Loss:31.45626520315806, Val Loss:26.568250135941938, 학습 시간: 33.875489711761475\n",
      "EPOCH:54, Train Loss:32.23313686450322, Val Loss:24.77866198799827, 학습 시간: 35.46911597251892\n",
      "EPOCH:55, Train Loss:34.073598353068036, Val Loss:26.20751840418035, 학습 시간: 35.54521298408508\n",
      "EPOCH:56, Train Loss:30.35326550801595, Val Loss:54.20743335377086, 학습 시간: 34.89288687705994\n",
      "EPOCH:57, Train Loss:32.4016523996989, Val Loss:33.812978050925516, 학습 시간: 34.55040740966797\n",
      "EPOCH:58, Train Loss:37.222225308418274, Val Loss:42.639436375011094, 학습 시간: 35.19183135032654\n",
      "EPOCH:59, Train Loss:46.42012972036998, Val Loss:12.13205692984841, 학습 시간: 35.677332639694214\n",
      "EPOCH:60, Train Loss:19.22078630924225, Val Loss:31.546971321105957, 학습 시간: 35.495057106018066\n",
      "EPOCH:61, Train Loss:31.513877081871033, Val Loss:24.111403378573332, 학습 시간: 34.61081647872925\n",
      "EPOCH:62, Train Loss:31.133535917599996, Val Loss:11.26998582753268, 학습 시간: 34.81570625305176\n",
      "EPOCH:63, Train Loss:23.75294318596522, Val Loss:11.85514428398826, 학습 시간: 35.254311084747314\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "model_dir = '/home/work/Tcae_apply/model_dir3/'\n",
    "min_loss = 987654321\n",
    "\n",
    "epochs = 150\n",
    "batch_size = 32\n",
    "\n",
    "train_batch_li = DataLoader(train_list, batch_size=batch_size, shuffle=True,drop_last=False)\n",
    "val_batch_li = DataLoader(valid_list, batch_size=batch_size, shuffle=True,drop_last=False)\n",
    "\n",
    "#tloss_list=[]\n",
    "#vloss_list=[]\n",
    "\n",
    "for epoch in tqdm(range(1, epochs + 1)):\n",
    "    start = time.time()\n",
    "    \n",
    "    \n",
    "    train_loss = train(model = model, train_loader = train_batch_li) \n",
    "    val_loss = val(model = model, train_loader = val_batch_li) \n",
    "    end = time.time()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #tloss_list.append(train_loss)\n",
    "    #vloss_list.append(val_loss)\n",
    "    \n",
    "    print(f'EPOCH:{epoch}, Train Loss:{train_loss}, Val Loss:{val_loss}, 학습 시간: {end - start}')\n",
    "    if (val_loss < min_loss) & (val_loss < 10) & (train_loss < 15) :\n",
    "        min_loss = val_loss\n",
    "        torch.save(model.state_dict(), model_dir + f'TimeAutoEncoder_skipconnection_charlie_val.pt')\n",
    "        print('모델 저장')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torchmetrics\n",
    "#from torchmetrics import MeanAbsolutePercentageError\n",
    "\n",
    "#model = TimeAutoEncoder().to(DEVICE)\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "#criterion = MeanAbsolutePercentageError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport time\\n\\nmodel_dir = '/home/work/Tcae_apply/model_dir3/'\\nmin_loss = 987654321\\n\\nepochs = 150\\nbatch_size = 64\\n\\ntrain_batch_li = DataLoader(train_list, batch_size=batch_size, shuffle=True,drop_last=True)\\nval_batch_li = DataLoader(valid_list, batch_size=batch_size, shuffle=True,drop_last=True)\\n\\n#tloss_list=[]\\n#vloss_list=[]\\n\\nfor epoch in tqdm(range(1, epochs + 1)):\\n    start = time.time()\\n    \\n    \\n    train_loss = train(model = model, train_loader = train_batch_li) \\n    val_loss = val(model = model, train_loader = val_batch_li) \\n    end = time.time()\\n    \\n    \\n    #tloss_list.append(train_loss)\\n    #vloss_list.append(val_loss)\\n    \\n    print(f'EPOCH:{epoch}, Train Loss:{train_loss}, Val Loss:{val_loss}, 학습 시간: {end - start}')\\n    if val_loss < min_loss:\\n        min_loss = val_loss\\n        torch.save(model.state_dict(), model_dir + f'TimeAutoEncoder_skipconnection_beta_val.pt')\\n        print('모델 저장')\\n        \\n\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import time\n",
    "\n",
    "model_dir = '/home/work/Tcae_apply/model_dir3/'\n",
    "min_loss = 987654321\n",
    "\n",
    "epochs = 150\n",
    "batch_size = 64\n",
    "\n",
    "train_batch_li = DataLoader(train_list, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "val_batch_li = DataLoader(valid_list, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "\n",
    "#tloss_list=[]\n",
    "#vloss_list=[]\n",
    "\n",
    "for epoch in tqdm(range(1, epochs + 1)):\n",
    "    start = time.time()\n",
    "    \n",
    "    \n",
    "    train_loss = train(model = model, train_loader = train_batch_li) \n",
    "    val_loss = val(model = model, train_loader = val_batch_li) \n",
    "    end = time.time()\n",
    "    \n",
    "    \n",
    "    #tloss_list.append(train_loss)\n",
    "    #vloss_list.append(val_loss)\n",
    "    \n",
    "    print(f'EPOCH:{epoch}, Train Loss:{train_loss}, Val Loss:{val_loss}, 학습 시간: {end - start}')\n",
    "    if val_loss < min_loss:\n",
    "        min_loss = val_loss\n",
    "        torch.save(model.state_dict(), model_dir + f'TimeAutoEncoder_skipconnection_beta_val.pt')\n",
    "        print('모델 저장')\n",
    "        \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir = '/home/work/Tcae_apply/model_dir3/'\n",
    "\n",
    "model = TimeAutoEncoder().to(DEVICE)\n",
    "model.load_state_dict(torch.load(model_dir + f'TimeAutoEncoder_skipconnection_charlie_val.pt', map_location = DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_li = DataLoader(all_list, batch_size=1, shuffle=False,drop_last=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mel_embeding_li = get_mel_embeding(model = model, train_loader = test_batch_li)\n",
    "mel_embeding = np.concatenate(mel_embeding_li, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(mel_embeding).to_csv('tcae_charlie_embedding.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save(batch_data_dir + 'mel_embeding_val.npy', mel_embeding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_npy_path = '/home/work/Tcae_apply/tcae_inference_folder/mel_folder/'\n",
    "inference_npy_path = glob.glob(inference_npy_path + '*.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/work/Tcae_apply/tcae_inference_folder/mel_folder/성현_보컬.npy',\n",
       " '/home/work/Tcae_apply/tcae_inference_folder/mel_folder/정하_보컬.npy',\n",
       " '/home/work/Tcae_apply/tcae_inference_folder/mel_folder/지평_보컬.npy']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_npy_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57e6ea3670c047d3b6b6559ccf032cfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inference_list = [np.load(song_path).squeeze()[:,:7501] for song_path in tqdm(inference_npy_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_batch_li = DataLoader(inference_list, batch_size=1, shuffle=False,drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mel_inference_embeding(model, train_loader):\n",
    "    model.eval()\n",
    "    mel_embeding_li = []\n",
    "    with torch.no_grad():\n",
    "        for batch in train_loader: \n",
    "            \n",
    "            mel = torch.FloatTensor(batch).to(DEVICE)\n",
    " \n",
    "            encode, output = model(mel)\n",
    "            mel_embeding_li.append(encode.detach().cpu().numpy())\n",
    "\n",
    "    return mel_embeding_li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inference_embedding_li = get_mel_embeding(model = model, train_loader = inference_batch_li)\n",
    "inference_embedding = np.concatenate(inference_embedding_li, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(inference_embedding).to_csv('inference_embedding1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 1.9.0 on Python 3.8 (CUDA 11.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
