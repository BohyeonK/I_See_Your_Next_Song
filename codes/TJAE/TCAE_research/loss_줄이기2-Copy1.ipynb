{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import torch \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re \n",
    "import os \n",
    "from torch.nn import functional as F\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "import gc\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "# residual connection\n",
    "from res_stack import ResStack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/home/work/'\n",
    "np1 = glob.glob(base_dir+'mel_dataset1/*.npy')\n",
    "np2 = glob.glob(base_dir+'mel_dataset2/*.npy')\n",
    "np3 = glob.glob(base_dir+'mel_dataset3/*.npy')\n",
    "np4 = glob.glob(base_dir+'mel_dataset4/*.npy')\n",
    "np5 = glob.glob(base_dir+'mel_dataset5/*.npy')\n",
    "np6 = glob.glob(base_dir+'mel_dataset6/*.npy')\n",
    "np7 = glob.glob(base_dir+'mel_dataset7/*.npy')\n",
    "np8 = glob.glob(base_dir+'mel_dataset8/*.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = np1+np2+np3+np4+np5+np6+np7+np8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv('new_train.csv')\n",
    "valid_csv = pd.read_csv('new_valid.csv')\n",
    "all_csv =pd.read_csv('new_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def npy_list(csv):\n",
    "#     npy_list = []\n",
    "#     for song_path in tqdm(csv['npy_path']):\n",
    "#         song_npy = np.load(song_path).squeeze()\n",
    "#         npy_list.append(song_npy)\n",
    "#     return npy_list\n",
    "\n",
    "# train_list = npy_list(train_csv)\n",
    "# valid_list = npy_list(valid_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75c5f7f977f843068e3469acc5985f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3825 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_list = [np.load(song_path).squeeze() for song_path in tqdm(train_csv['npy_path'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1720bdc660d4c598338385d6e99b056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/675 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "valid_list = [np.load(song_path).squeeze() for song_path in tqdm(valid_csv['npy_path'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_list = train_list + valid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "epochs = 150\n",
    "batch_size = 32\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((48, 7501), (48, 7501))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_list[0].shape, valid_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch문 안에 들어가야되나?\n",
    "#train_batch_li = DataLoader(train_list, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "#val_batch_li = DataLoader(valid_list, batch_size=batch_size, shuffle=True,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# denoise encoding\n",
    "def add_noise(data):\n",
    "    noise = torch.randn(data.size()) * 0.2 \n",
    "    # 무작위 작음은 torch.randn() 함수로 만들고 img.size()를 넣어 이미지와 같은 크기의 잡음을 만듭니다.\n",
    "    # 잡음의 강도는 임의로 0.2로 정했습니다.\n",
    "    noisy_data = data + noise\n",
    "    return noisy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in train_loader: # 배치파일 다 만들면 train_loader [] 삭제하기\n",
    "        \n",
    "        #mel = np.load(batch_name) \n",
    "        batch = add_noise(batch) # denoise encoding\n",
    "        mel = torch.FloatTensor(batch).to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        encode, output = model(mel)\n",
    "        \n",
    "        loss = criterion(output, mel)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    train_loss /= (len(train_loader))\n",
    "\n",
    "    return train_loss\n",
    "\n",
    "def val(model, train_loader):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in train_loader:# 배치파일 다 만들면 train_loader [] 삭제하기\n",
    "            \n",
    "            #mel = np.load(batch_name)\n",
    "            mel = torch.FloatTensor(batch).to(DEVICE)\n",
    "\n",
    "            encode, output = model(mel)\n",
    "\n",
    "            loss = criterion(output, mel)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= (len(train_loader))\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "def get_mel_embeding(model, train_loader):\n",
    "    model.eval()\n",
    "    mel_embeding_li = []\n",
    "    with torch.no_grad():\n",
    "        for batch in train_loader: # 배치파일 다 만들면 train_loader [] 삭제하기\n",
    "            \n",
    "            #mel = np.load(batch_name)\n",
    "            mel = torch.FloatTensor(batch).to(DEVICE)\n",
    "            \n",
    "            encode, output = model(mel)\n",
    "            mel_embeding_li.append(encode.detach().cpu().numpy())\n",
    "\n",
    "    return mel_embeding_li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Attention - 수정이 필요함... 정신이 혼미해\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "class channel_attention(nn.Module):    \n",
    "    def __init__(self, channel, reduction=4):\n",
    "        super(channel_attention, self).__init__()\n",
    "        \n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x): \n",
    "        b, c, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1)\n",
    "        \n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "class spatial_attention(nn.Module):   \n",
    "    def __init__(self, channel, k_size=3):\n",
    "        super(spatial_attention, self).__init__()\n",
    "        self.conv = nn.Conv2d(channel, 1, kernel_size=k_size, padding='same', bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "            \n",
    "    def forward(self, x):\n",
    "        y = self.conv(x)\n",
    "        y = self.sigmoid(y)\n",
    "    \n",
    "        return x * y.expand_as(x)\n",
    "    \n",
    "    \n",
    "class attention_module(nn.Module):\n",
    "    def __init__(self, channel, attention_type=None):\n",
    "        super(attention_module, self).__init__()\n",
    "        self.CA = channel_attention(channel)\n",
    "        self.SA = spatial_attention(channel)\n",
    "        self.SC = nn.Sequential(\n",
    "            channel_attention(channel),\n",
    "            spatial_attention(channel),\n",
    "        )\n",
    "        self.SS = nn.Sequential(\n",
    "            spatial_attention(channel),\n",
    "            channel_attention(channel),\n",
    "        )\n",
    "        self.attention_type = attention_type\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.attention_type == 'channel':\n",
    "            output = self.CA(x)\n",
    "            \n",
    "        elif self.attention_type == 'spatial':\n",
    "            output = self.SA(x)\n",
    "            \n",
    "        elif self.attention_type == 'parellel_add':\n",
    "            output1 = self.CA(x)\n",
    "            output2 = self.SA(x)\n",
    "            output = output1 + output2\n",
    "            \n",
    "        elif self.attention_type == 'parellel_mul':\n",
    "            output1 = self.CA(x)\n",
    "            output2 = self.SA(x)\n",
    "            output = output1 * output2\n",
    "            \n",
    "        elif self.attention_type == 'serial_ca':\n",
    "            output = self.SC(x)\n",
    "            \n",
    "        elif self.attention_type == 'serial_sa':\n",
    "            output = self.SS(x)\n",
    "\n",
    "        else:\n",
    "            output  = x\n",
    "    \n",
    "        return output\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# encoder <- 과거의 데이터를 바탕으로 미래를 예측 (t-1의 인과성 학습)\n",
    "# decoder <- 미래의 데이터를 바탕으로 과거를 예측 (t+1의 인과성 학습)\n",
    "class TimeAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TimeAutoEncoder, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 48, out_channels = 512, kernel_size = 3, stride = 1, padding = 0, dilation = 1),\n",
    "            channel_attention(512), \n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 512, out_channels = 256, kernel_size = 3, stride = 1, padding = 0, dilation = 2),\n",
    "            channel_attention(256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 256, out_channels = 128, kernel_size = 3, stride = 1, padding = 0, dilation = 4),\n",
    "            channel_attention(128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 128, out_channels = 64, kernel_size = 3, stride = 1, padding = 0, dilation = 8),\n",
    "            channel_attention(64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 64, out_channels = 32, kernel_size = 3, stride = 1, padding = 0, dilation = 16),\n",
    "            channel_attention(32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv6 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 32, out_channels = 16, kernel_size = 3, stride = 1, padding = 0, dilation = 32),\n",
    "            channel_attention(16),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv7 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 16, out_channels = 8, kernel_size = 3, stride = 1, padding = 0, dilation = 64),\n",
    "            channel_attention(8),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.encoder_fc = nn.Sequential(\n",
    "            nn.Linear(8 * 7501, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "        self.decoder_fc = nn.Sequential(\n",
    "            nn.Linear(128, 8 * 7501),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.t_conv1 = nn.Sequential(\n",
    "            # nn.ConvTranspose1d(in_channels = 8, out_channels = 16, kernel_size  = 3, stride = 1, dilation=62),\n",
    "            nn.Conv1d(in_channels = 8, out_channels = 16, kernel_size = 3, stride = 1, padding = 0, dilation = 64),\n",
    "            channel_attention(16),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.t_conv2 = nn.Sequential(\n",
    "            # nn.ConvTranspose1d(in_channels = 16, out_channels = 32, kernel_size  = 3, stride = 1, dilation = 30),\n",
    "            nn.Conv1d(in_channels = 16, out_channels = 32, kernel_size = 3, stride = 1, padding = 0, dilation = 32),\n",
    "            channel_attention(32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.t_conv3 = nn.Sequential(\n",
    "            # nn.ConvTranspose1d(in_channels = 32, out_channels = 64, kernel_size  = 3, stride = 1, dilation=14),\n",
    "            nn.Conv1d(in_channels = 32, out_channels = 64, kernel_size = 3, stride = 1, padding = 0, dilation = 16),\n",
    "            channel_attention(64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.t_conv4 = nn.Sequential(\n",
    "            # nn.ConvTranspose1d(in_channels = 64, out_channels = 128, kernel_size  = 3, stride = 1, dilation = 6),\n",
    "            nn.Conv1d(in_channels = 64, out_channels = 128, kernel_size = 3, stride = 1, padding = 0, dilation = 8),\n",
    "            channel_attention(128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.t_conv5 = nn.Sequential(\n",
    "            # nn.ConvTranspose1d(in_channels = 128, out_channels = 256, kernel_size  = 3, stride = 1, dilation=2),\n",
    "            nn.Conv1d(in_channels = 128, out_channels = 256, kernel_size = 3, stride = 1, padding = 0, dilation = 4),\n",
    "            channel_attention(256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.t_conv6 = nn.Sequential(\n",
    "            # nn.ConvTranspose1d(in_channels = 256, out_channels = 512, kernel_size  = 3, stride = 1, dilation = 1),\n",
    "            nn.Conv1d(in_channels = 256, out_channels = 512, kernel_size = 3, stride = 1, padding = 0, dilation = 2),\n",
    "            channel_attention(512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.t_conv7 = nn.Sequential(\n",
    "            # nn.ConvTranspose1d(in_channels = 512, out_channels = 48, kernel_size  = 3, stride = 1, dilation= 1),\n",
    "            nn.Conv1d(in_channels = 512, out_channels = 48, kernel_size = 3, stride = 1, padding = 0, dilation = 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, mel_spec):\n",
    "        x = F.pad(mel_spec, pad = (2, 0, 0, 0))\n",
    "        x = self.conv1(x)\n",
    "        #print(x.shape)\n",
    "        x = F.pad(x, pad = (4, 0, 0, 0))\n",
    "        x = self.conv2(x)\n",
    "        #print(x.shape)\n",
    "        x = F.pad(x, pad = (8, 0, 0, 0))\n",
    "        x = self.conv3(x)\n",
    "        #print(x.shape)\n",
    "        x = F.pad(x, pad = (16, 0, 0, 0))\n",
    "        x = self.conv4(x)\n",
    "        #print(x.shape)\n",
    "        x = F.pad(x, pad = (32, 0, 0, 0))\n",
    "        x = self.conv5(x)\n",
    "        #print(x.shape)\n",
    "        x = F.pad(x, pad = (64, 0, 0, 0))\n",
    "        x = self.conv6(x)\n",
    "        #print(x.shape)\n",
    "        x = F.pad(x, pad = (128, 0, 0, 0))\n",
    "        x = self.conv7(x)\n",
    "        #print(x.shape)\n",
    "        encode = self.encoder_fc(x.view(-1, 8 * 7501))\n",
    "\n",
    "        #encode = self.encoder_fc(x.view(-1, 8 * 1876))\n",
    "\n",
    "        # print('decode')\n",
    "        x = self.decoder_fc(encode)\n",
    "        x = x.view(-1, 8, 7501)\n",
    "        x = torch.swapaxes(torch.fliplr(torch.swapaxes(x, 1, 2)), 1, 2)\n",
    "        x = F.pad(x, pad = (128, 0, 0, 0))\n",
    "        x = self.t_conv1(x)\n",
    "        #print(x.shape)\n",
    "        x = F.pad(x, pad = (64, 0, 0, 0))\n",
    "        x = self.t_conv2(x)\n",
    "        #print(x.shape)\n",
    "        x = F.pad(x, pad = (32, 0, 0, 0))\n",
    "        x = self.t_conv3(x)\n",
    "        #print(x.shape)\n",
    "        x = F.pad(x, pad = (16, 0, 0, 0))\n",
    "        x = self.t_conv4(x)\n",
    "        #print(x.shape)\n",
    "        x = F.pad(x, pad = (8, 0, 0, 0))\n",
    "        x = self.t_conv5(x)\n",
    "        #print(x.shape)\n",
    "        x = F.pad(x, pad = (4, 0, 0, 0))\n",
    "        x = self.t_conv6(x)\n",
    "        #print(x.shape)\n",
    "        x = F.pad(x, pad = (2, 0, 0, 0))\n",
    "        x = self.t_conv7(x)\n",
    "        #print(x.shape)\n",
    "        x = torch.swapaxes(torch.fliplr(torch.swapaxes(x, 1, 2)), 1, 2)\n",
    "        \n",
    "        return encode, x\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResStack 추가\n",
    "# LeakyReLU로 변경\n",
    "# encoder <- 과거의 데이터를 바탕으로 미래를 예측 (t-1의 인과성 학습)\n",
    "# decoder <- 미래의 데이터를 바탕으로 과거를 예측 (t+1의 인과성 학습)\n",
    "class TimeAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TimeAutoEncoder, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 48, out_channels = 512, kernel_size = 3, stride = 1, padding = 0, dilation = 1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(), \n",
    "        )\n",
    "        ResStack(512), \n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 512, out_channels = 256, kernel_size = 3, stride = 1, padding = 0, dilation = 2),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        ResStack(256),\n",
    "        \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 256, out_channels = 128, kernel_size = 3, stride = 1, padding = 0, dilation = 4),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        ResStack(128),\n",
    "        \n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 128, out_channels = 64, kernel_size = 3, stride = 1, padding = 0, dilation = 8),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        ResStack(64),\n",
    "        \n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 64, out_channels = 32, kernel_size = 3, stride = 1, padding = 0, dilation = 16),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        ResStack(32),\n",
    "        \n",
    "        self.conv6 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 32, out_channels = 16, kernel_size = 3, stride = 1, padding = 0, dilation = 32),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        ResStack(16),\n",
    "        \n",
    "        self.conv7 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 16, out_channels = 8, kernel_size = 3, stride = 1, padding = 0, dilation = 64),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        ResStack(8),\n",
    "        \n",
    "        self.encoder_fc = nn.Sequential(\n",
    "            nn.Linear(8 * 7501, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "        self.decoder_fc = nn.Sequential(\n",
    "            nn.Linear(128, 8 * 7501),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        \n",
    "        ResStack(8),\n",
    "        self.t_conv1 = nn.Sequential(\n",
    "            # nn.ConvTranspose1d(in_channels = 8, out_channels = 16, kernel_size  = 3, stride = 1, dilation=62),\n",
    "            nn.Conv1d(in_channels = 8, out_channels = 16, kernel_size = 3, stride = 1, padding = 0, dilation = 64),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "        ResStack(16),\n",
    "        self.t_conv2 = nn.Sequential(\n",
    "            # nn.ConvTranspose1d(in_channels = 16, out_channels = 32, kernel_size  = 3, stride = 1, dilation = 30),\n",
    "            nn.Conv1d(in_channels = 16, out_channels = 32, kernel_size = 3, stride = 1, padding = 0, dilation = 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "        ResStack(32),\n",
    "        self.t_conv3 = nn.Sequential(\n",
    "            # nn.ConvTranspose1d(in_channels = 32, out_channels = 64, kernel_size  = 3, stride = 1, dilation=14),\n",
    "            nn.Conv1d(in_channels = 32, out_channels = 64, kernel_size = 3, stride = 1, padding = 0, dilation = 16),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "        ResStack(64),\n",
    "        self.t_conv4 = nn.Sequential(\n",
    "            # nn.ConvTranspose1d(in_channels = 64, out_channels = 128, kernel_size  = 3, stride = 1, dilation = 6),\n",
    "            nn.Conv1d(in_channels = 64, out_channels = 128, kernel_size = 3, stride = 1, padding = 0, dilation = 8),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "        ResStack(128),\n",
    "        self.t_conv5 = nn.Sequential(\n",
    "            # nn.ConvTranspose1d(in_channels = 128, out_channels = 256, kernel_size  = 3, stride = 1, dilation=2),\n",
    "            nn.Conv1d(in_channels = 128, out_channels = 256, kernel_size = 3, stride = 1, padding = 0, dilation = 4),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "        ResStack(256),\n",
    "        self.t_conv6 = nn.Sequential(\n",
    "            # nn.ConvTranspose1d(in_channels = 256, out_channels = 512, kernel_size  = 3, stride = 1, dilation = 1),\n",
    "            nn.Conv1d(in_channels = 256, out_channels = 512, kernel_size = 3, stride = 1, padding = 0, dilation = 2),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "        ResStack(512),\n",
    "        self.t_conv7 = nn.Sequential(\n",
    "            # nn.ConvTranspose1d(in_channels = 512, out_channels = 48, kernel_size  = 3, stride = 1, dilation= 1),\n",
    "            nn.Conv1d(in_channels = 512, out_channels = 48, kernel_size = 3, stride = 1, padding = 0, dilation = 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, mel_spec):\n",
    "        x = F.pad(mel_spec, pad = (2, 0, 0, 0))\n",
    "        x = self.conv1(x)\n",
    "        #print(x.shape)\n",
    "        x = F.pad(x, pad = (4, 0, 0, 0))\n",
    "        x = self.conv2(x)\n",
    "        #print(x.shape)\n",
    "        x = F.pad(x, pad = (8, 0, 0, 0))\n",
    "        x = self.conv3(x)\n",
    "        #print(x.shape)\n",
    "        x = F.pad(x, pad = (16, 0, 0, 0))\n",
    "        x = self.conv4(x)\n",
    "        #print(x.shape)\n",
    "        x = F.pad(x, pad = (32, 0, 0, 0))\n",
    "        x = self.conv5(x)\n",
    "        #print(x.shape)\n",
    "        x = F.pad(x, pad = (64, 0, 0, 0))\n",
    "        x = self.conv6(x)\n",
    "        #print(x.shape)\n",
    "        x = F.pad(x, pad = (128, 0, 0, 0))\n",
    "        x = self.conv7(x)\n",
    "        #print(x.shape)\n",
    "        encode = self.encoder_fc(x.view(-1, 8 * 7501))\n",
    "\n",
    "        #encode = self.encoder_fc(x.view(-1, 8 * 1876))\n",
    "\n",
    "        # print('decode')\n",
    "        x = self.decoder_fc(encode)\n",
    "        x = x.view(-1, 8, 7501)\n",
    "        x = torch.swapaxes(torch.fliplr(torch.swapaxes(x, 1, 2)), 1, 2)\n",
    "        x = F.pad(x, pad = (128, 0, 0, 0))\n",
    "        x = self.t_conv1(x)\n",
    "        #print(x.shape)\n",
    "        x = F.pad(x, pad = (64, 0, 0, 0))\n",
    "        x = self.t_conv2(x)\n",
    "        #print(x.shape)\n",
    "        x = F.pad(x, pad = (32, 0, 0, 0))\n",
    "        x = self.t_conv3(x)\n",
    "        #print(x.shape)\n",
    "        x = F.pad(x, pad = (16, 0, 0, 0))\n",
    "        x = self.t_conv4(x)\n",
    "        #print(x.shape)\n",
    "        x = F.pad(x, pad = (8, 0, 0, 0))\n",
    "        x = self.t_conv5(x)\n",
    "        #print(x.shape)\n",
    "        x = F.pad(x, pad = (4, 0, 0, 0))\n",
    "        x = self.t_conv6(x)\n",
    "        #print(x.shape)\n",
    "        x = F.pad(x, pad = (2, 0, 0, 0))\n",
    "        x = self.t_conv7(x)\n",
    "        #print(x.shape)\n",
    "        x = torch.swapaxes(torch.fliplr(torch.swapaxes(x, 1, 2)), 1, 2)\n",
    "        \n",
    "        return encode, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TimeAutoEncoder().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "criterion = nn.L1Loss() # MSE에서 MAE로 바꿈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_list_shrt = [i[:,-3752:] for i in train_list]\n",
    "# valid_list_shrt = [i[:,-3752:] for i in valid_list]\n",
    "\n",
    "# train_list_shrt[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "416a7358d884470181d6c9e5031950ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH:1, Train Loss:20.331643921988352, Val Loss:19.834490004039946, 학습 시간: 36.404571771621704\n",
      "모델 저장\n",
      "EPOCH:2, Train Loss:20.26914045189609, Val Loss:19.88207163129534, 학습 시간: 36.76791453361511\n",
      "EPOCH:3, Train Loss:20.232004197705695, Val Loss:19.796177046639578, 학습 시간: 36.79360914230347\n",
      "모델 저장\n",
      "EPOCH:4, Train Loss:20.18888117686039, Val Loss:19.7868595123291, 학습 시간: 36.99477672576904\n",
      "모델 저장\n",
      "EPOCH:5, Train Loss:20.157301918799135, Val Loss:19.66015238988967, 학습 시간: 36.9385769367218\n",
      "모델 저장\n",
      "EPOCH:6, Train Loss:20.099749981856146, Val Loss:19.64265841529483, 학습 시간: 36.753530502319336\n",
      "모델 저장\n",
      "EPOCH:7, Train Loss:20.009544428657083, Val Loss:19.642210097539994, 학습 시간: 36.98733329772949\n",
      "모델 저장\n",
      "EPOCH:8, Train Loss:19.89348023679076, Val Loss:19.571494602021716, 학습 시간: 37.03239560127258\n",
      "모델 저장\n",
      "EPOCH:9, Train Loss:19.82877737734498, Val Loss:19.664703051249187, 학습 시간: 38.1425244808197\n",
      "EPOCH:10, Train Loss:19.704275379661752, Val Loss:19.58768131619408, 학습 시간: 37.34929633140564\n",
      "EPOCH:11, Train Loss:19.573714793229303, Val Loss:19.5249205998012, 학습 시간: 37.03777313232422\n",
      "모델 저장\n",
      "EPOCH:12, Train Loss:19.483575684683665, Val Loss:19.436796188354492, 학습 시간: 36.93436408042908\n",
      "모델 저장\n",
      "EPOCH:13, Train Loss:19.37962454306979, Val Loss:19.639873050508044, 학습 시간: 36.864577293395996\n",
      "EPOCH:14, Train Loss:19.255912436156713, Val Loss:19.634787332443963, 학습 시간: 37.145877838134766\n",
      "EPOCH:15, Train Loss:19.17114404469979, Val Loss:19.6328098206293, 학습 시간: 36.84852647781372\n",
      "EPOCH:16, Train Loss:19.036531504462747, Val Loss:19.67875557854062, 학습 시간: 36.70821976661682\n",
      "EPOCH:17, Train Loss:18.976847664648744, Val Loss:19.643419356573197, 학습 시간: 36.85485315322876\n",
      "EPOCH:18, Train Loss:18.856149681475983, Val Loss:19.563486553373792, 학습 시간: 36.895912408828735\n",
      "EPOCH:20, Train Loss:18.654577824247987, Val Loss:19.529374168032692, 학습 시간: 36.844791889190674\n",
      "EPOCH:21, Train Loss:18.594705044722357, Val Loss:19.651504562014626, 학습 시간: 36.90622854232788\n",
      "EPOCH:22, Train Loss:18.514494358992376, Val Loss:19.586197943914506, 학습 시간: 37.216840744018555\n",
      "EPOCH:23, Train Loss:18.43589612816562, Val Loss:19.616889862787154, 학습 시간: 36.89929795265198\n",
      "EPOCH:24, Train Loss:18.33952292274026, Val Loss:19.708546093532018, 학습 시간: 37.226627588272095\n",
      "EPOCH:25, Train Loss:18.23537743993166, Val Loss:19.661548024132138, 학습 시간: 37.04473614692688\n",
      "EPOCH:26, Train Loss:18.16904558454241, Val Loss:19.66925898052397, 학습 시간: 36.93494939804077\n",
      "EPOCH:27, Train Loss:18.072927971847918, Val Loss:19.753440493629093, 학습 시간: 37.089258909225464\n",
      "EPOCH:28, Train Loss:17.973648640288026, Val Loss:19.600646881830123, 학습 시간: 36.688796520233154\n",
      "EPOCH:29, Train Loss:17.932663933569643, Val Loss:19.78145186106364, 학습 시간: 36.51586174964905\n",
      "EPOCH:30, Train Loss:17.79938446974554, Val Loss:19.68620527358282, 학습 시간: 36.78759574890137\n",
      "EPOCH:31, Train Loss:17.72866071172121, Val Loss:19.52946404048375, 학습 시간: 36.82397699356079\n",
      "EPOCH:32, Train Loss:17.68240961507589, Val Loss:19.626524425688245, 학습 시간: 36.98194122314453\n",
      "EPOCH:33, Train Loss:17.630994003360012, Val Loss:19.646337418329146, 학습 시간: 37.79202365875244\n",
      "EPOCH:34, Train Loss:17.54144089963256, Val Loss:19.575748534429643, 학습 시간: 37.03501105308533\n",
      "EPOCH:35, Train Loss:17.49731148791914, Val Loss:19.738109906514484, 학습 시간: 36.47353720664978\n",
      "EPOCH:36, Train Loss:17.39371952088941, Val Loss:19.64909067608061, 학습 시간: 36.788997173309326\n",
      "EPOCH:37, Train Loss:17.327427992299825, Val Loss:19.79635865347726, 학습 시간: 36.61779856681824\n",
      "EPOCH:38, Train Loss:17.31198791696244, Val Loss:20.094552902948287, 학습 시간: 36.98215389251709\n",
      "EPOCH:39, Train Loss:17.276607946187507, Val Loss:19.672466005597794, 학습 시간: 36.93527340888977\n",
      "EPOCH:40, Train Loss:17.190038456636316, Val Loss:19.64238893418085, 학습 시간: 36.61847424507141\n",
      "EPOCH:41, Train Loss:17.12249679725711, Val Loss:19.868798028855096, 학습 시간: 36.93618583679199\n",
      "EPOCH:42, Train Loss:17.067618033465216, Val Loss:19.812711488632928, 학습 시간: 37.132402420043945\n",
      "EPOCH:43, Train Loss:17.028128343469955, Val Loss:19.661596116565523, 학습 시간: 36.37771940231323\n",
      "EPOCH:44, Train Loss:16.999542813341158, Val Loss:19.91703968956357, 학습 시간: 36.83702111244202\n",
      "EPOCH:45, Train Loss:16.978014136562827, Val Loss:19.640161968412855, 학습 시간: 36.88343787193298\n",
      "EPOCH:46, Train Loss:16.91170117835037, Val Loss:19.757457914806547, 학습 시간: 37.14674115180969\n",
      "EPOCH:47, Train Loss:16.85754756767209, Val Loss:19.679518063863117, 학습 시간: 37.00031661987305\n",
      "EPOCH:48, Train Loss:16.82453250083603, Val Loss:19.792027972993395, 학습 시간: 37.075700998306274\n",
      "EPOCH:49, Train Loss:16.740140113510005, Val Loss:19.82415771484375, 학습 시간: 37.17675733566284\n",
      "EPOCH:50, Train Loss:16.727477843020143, Val Loss:19.766739618210565, 학습 시간: 37.19422245025635\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "model_dir = '/home/work/Tcae_apply/model_dir2/'\n",
    "min_loss = 987654321\n",
    "\n",
    "#train_batch_li = DataLoader(train_list, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "#val_batch_li = DataLoader(valid_list, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "\n",
    "for epoch in tqdm(range(1, epochs + 1)):\n",
    "    start = time.time()\n",
    "    \n",
    "    train_batch_li = DataLoader(train_list, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "    val_batch_li = DataLoader(valid_list, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "\n",
    "    train_loss = train(model = model, train_loader = train_batch_li) \n",
    "    val_loss = val(model = model, train_loader = val_batch_li) \n",
    "    end = time.time()\n",
    "\n",
    "    print(f'EPOCH:{epoch}, Train Loss:{train_loss}, Val Loss:{val_loss}, 학습 시간: {end - start}')\n",
    "    if val_loss < min_loss:\n",
    "        min_loss = val_loss\n",
    "        torch.save(model.state_dict(), model_dir + f'TimeAutoEncoder_val.pt')\n",
    "        print('모델 저장')\n",
    "        \n",
    "    del train_batch_li\n",
    "    del val_batch_li\n",
    " # Resstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc94da8f17dc49a7a2262217b246b5bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH:1, Train Loss:20.34235031865224, Val Loss:19.95621054513114, 학습 시간: 32.697444677352905\n",
      "모델 저장\n",
      "EPOCH:2, Train Loss:20.26667368912897, Val Loss:19.857975823538645, 학습 시간: 32.84595990180969\n",
      "모델 저장\n",
      "EPOCH:3, Train Loss:20.256751228781308, Val Loss:19.868038813273113, 학습 시간: 33.06649208068848\n",
      "EPOCH:4, Train Loss:20.216806211391418, Val Loss:19.84003457568941, 학습 시간: 33.23177766799927\n",
      "모델 저장\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-261deadfb7e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m#val_batch_li = DataLoader(valid_list, batch_size=batch_size, shuffle=True,drop_last=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_batch_li\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_batch_li\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-73-8d76b5af8258>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mencode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-88-d7421c9a9564>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, mel_spec)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_conv7\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;31m#print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfliplr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "import time\n",
    "\n",
    "model_dir = '/home/work/Tcae_apply/model_dir/'\n",
    "min_loss = 987654321\n",
    "\n",
    "train_batch_li = DataLoader(train_list, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "val_batch_li = DataLoader(valid_list, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "\n",
    "for epoch in tqdm(range(1, epochs + 1)):\n",
    "    start = time.time()\n",
    "    \n",
    "    #train_batch_li = DataLoader(train_list, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "    #val_batch_li = DataLoader(valid_list, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "\n",
    "    train_loss = train(model = model, train_loader = train_batch_li) \n",
    "    val_loss = val(model = model, train_loader = val_batch_li) \n",
    "    end = time.time()\n",
    "\n",
    "    print(f'EPOCH:{epoch}, Train Loss:{train_loss}, Val Loss:{val_loss}, 학습 시간: {end - start}')\n",
    "    if val_loss < min_loss:\n",
    "        min_loss = val_loss\n",
    "        torch.save(model.state_dict(), model_dir + f'TimeAutoEncoder_val.pt')\n",
    "        print('모델 저장')\n",
    "        \n",
    "    #del train_batch_li\n",
    "    #del valid_batch_li\n",
    "# leaky \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = TimeAutoEncoder().to(DEVICE)\n",
    "#model.load_state_dict(torch.load(model_dir + f'TimeAutoEncoder_val.pt', map_location = DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mel_embeding_li = get_mel_embeding(model = model, train_loader = test_batch_li)\n",
    "#mel_embeding = np.concatenate(mel_embeding_li, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save(batch_data_dir + 'mel_embeding_val.npy', mel_embeding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 1.9.0 on Python 3.8 (CUDA 11.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
